# Story 5.1: SeaweedFS Object Storage

## Status
Approved

## Story
**As a** storage administrator,
**I want** SeaweedFS 3.97 deployed with S3 compatibility,
**so that** all applications have reliable object storage with standard APIs.

## Acceptance Criteria
1. SeaweedFS container running with specified version
2. S3 API compatibility enabled and working
3. Volume and server topology configured
4. Replication and redundancy setup
5. Storage quotas and limits configured
6. Basic file upload/download tested

## Tasks / Subtasks

- [ ] **Task 0: Verify Prerequisites and Plan Migration for Existing Services** (AC: 1)
  - [ ] Verify Caddy container is running and healthy: `docker compose ps caddy | grep "healthy"`
  - [ ] Identify services currently using local volume storage:
    - Directus: `borgstack_directus_uploads` (Story 4.1)
    - FileFlows: `borgstack_fileflows_input`, `borgstack_fileflows_output` (Story 4.2)
    - n8n: `borgstack_n8n_data` (may contain workflow attachments)
  - [ ] Review S3 migration templates created in previous stories:
    - `config/directus/s3-storage.env.example`
    - `config/fileflows/s3-storage.env.example`
  - [ ] Document migration plan: Deploy SeaweedFS first, then migrate services in Story 5.3
  - [ ] **CHECKPOINT:** Confirm all prerequisites met before proceeding

- [ ] **Task 1: Add SeaweedFS Service to docker-compose.yml** (AC: 1, 3)
  - [ ] Add SeaweedFS service definition using `chrislusf/seaweedfs:3.97` image
  - [ ] Configure service on `borgstack_internal` network only (no external access)
  - [ ] Use single-server deployment mode: `weed server -filer -s3` command
    - Starts master, volume, filer, and S3 API in one container
    - Suitable for single-server BorgStack deployment
  - [ ] Mount persistent volumes:
    - `borgstack_seaweedfs_master:/data/master` for master metadata
    - `borgstack_seaweedfs_volume:/data/volume` for actual file storage
    - `borgstack_seaweedfs_filer:/data/filer` for filer database
  - [ ] Expose internal ports:
    - 9333: Master API (volume allocation, topology management)
    - 8080: Volume server API (file read/write operations)
    - 8888: Filer API (file system operations)
    - 8333: S3 API (S3-compatible object storage)
  - [ ] Set restart policy: `unless-stopped`
  - [ ] Configure container name: `seaweedfs`
  - [ ] Add dependency on no other services (foundational infrastructure)

- [ ] **Task 2: Configure SeaweedFS Environment Variables** (AC: 1, 3, 4)
  - [ ] Set `WEED_MASTER_VOLUME_SIZE_LIMIT_MB=10240` (10GB max volume size)
  - [ ] Set `WEED_MASTER_DEFAULT_REPLICATION=000` (no replication for single server)
    - Format: `XYZ` where X=datacenter copies, Y=rack copies, Z=server copies
    - `000` = no replication (single server mode)
    - Future: Change to `001` or higher when adding servers
  - [ ] Configure S3 credentials:
    - `AWS_ACCESS_KEY_ID=${SEAWEEDFS_ACCESS_KEY}` from .env
    - `AWS_SECRET_ACCESS_KEY=${SEAWEEDFS_SECRET_KEY}` from .env
  - [ ] Set filer configuration:
    - `WEED_FILER_OPTIONS=-defaultReplicaPlacement=000`
  - [ ] Set volume pre-allocation (optional):
    - `WEED_VOLUME_PREALLOCATE=true` for better performance

- [ ] **Task 3: Implement SeaweedFS Health Check** (AC: 1)
  - [ ] Add health check using `curl -f http://localhost:9333/cluster/status || exit 1`
  - [ ] Verify master API responds with cluster status
  - [ ] Set health check interval to 30 seconds
  - [ ] Set timeout to 10 seconds
  - [ ] Set retries to 5 before marking unhealthy
  - [ ] Set start_period to 60 seconds (cluster initialization)

- [ ] **Task 4: Update .env.example with SeaweedFS Variables** (AC: 2, 4, 5)
  - [ ] Add S3 credentials section: "SeaweedFS S3-Compatible Object Storage"
  - [ ] Add `SEAWEEDFS_ACCESS_KEY=<generate-random-32-char>` with generation instructions
  - [ ] Add `SEAWEEDFS_SECRET_KEY=<generate-random-64-char>` with generation instructions
  - [ ] Add volume configuration:
    - `SEAWEEDFS_VOLUME_SIZE_LIMIT_MB=10240` with comment "Max size per volume in MB (default: 10GB)"
    - `SEAWEEDFS_REPLICATION=000` with comment "Single server: 000, Multi-server: 001+ (XYZ format)"
  - [ ] Add storage quota configuration (optional):
    - `SEAWEEDFS_MAX_VOLUMES=100` with comment "Maximum number of volumes (default: 100)"
  - [ ] Add note: "S3 endpoint will be available at http://seaweedfs:8333 (internal network only)"

- [ ] **Task 5: Create SeaweedFS Configuration Directory** (AC: 2, 3, 4, 5)
  - [ ] Create `config/seaweedfs/` directory
  - [ ] Create `config/seaweedfs/README.md` with setup instructions
  - [ ] Document S3 API usage:
    - S3 endpoint: `http://seaweedfs:8333` (internal) or `https://${SEAWEEDFS_HOST}` (if exposing via Caddy)
    - Bucket structure: `/buckets/{bucket-name}/`
    - Compatible with AWS SDK, s3cmd, and other S3 clients
  - [ ] Document bucket organization strategy (from architecture):
    ```
    /buckets/
      borgstack/
        n8n/              # n8n workflow attachments
        chatwoot/         # Chatwoot attachments
        directus/         # Directus CMS assets
        fileflows/        # FileFlows processed media
        lowcoder/         # Lowcoder app assets
        duplicati/        # Backup staging
    ```
  - [ ] Create filer configuration template `config/seaweedfs/filer.toml`:
    - Configure default storage (LevelDB for single server)
    - Set max connection pool
    - Configure directory quota settings
  - [ ] Document replication strategy:
    - Single server: `000` (no replication)
    - Multi-server expansion: Change to `001` or higher
  - [ ] Document volume management commands:
    - Check cluster status: `curl http://localhost:9333/cluster/status`
    - Grow volumes: `curl "http://localhost:9333/vol/grow?count=4"`
    - Check volume topology: `curl http://localhost:9333/dir/status`

- [ ] **Task 6: Create SeaweedFS Deployment Verification Test** (AC: 1-6)
  - [ ] Create `tests/deployment/verify-seaweedfs.sh` test script
  - [ ] Test 1: Verify SeaweedFS container is running: `docker compose ps seaweedfs | grep -q "Up"`
  - [ ] Test 2: Verify correct image version: `docker compose ps seaweedfs | grep -q "chrislusf/seaweedfs:3.97"`
  - [ ] Test 3: Verify health check is passing: `docker compose ps seaweedfs | grep -q "healthy"`
  - [ ] Test 4: Verify master API is accessible: `curl -f http://localhost:9333/cluster/status`
  - [ ] Test 5: Verify volume server API: `curl -f http://localhost:8080/status`
  - [ ] Test 6: Verify filer API: `curl -f http://localhost:8888/`
  - [ ] Test 7: Verify S3 API endpoint: `curl -f http://localhost:8333/`
  - [ ] Test 8: Verify volumes are mounted and writable
  - [ ] Test 9: Check initial volume allocation: `curl http://localhost:9333/dir/status`
  - [ ] Test 10: Verify network isolation (`borgstack_internal` only)
  - [ ] Test 11: Test S3 bucket creation using AWS CLI or s3cmd
  - [ ] Test 12: Test file upload to S3 (AC: 6)
  - [ ] Test 13: Test file download from S3 (AC: 6)
  - [ ] Test 14: Verify replication strategy is configured correctly
  - [ ] Test 15: Check SeaweedFS logs for startup errors
  - [ ] Make script executable: `chmod +x tests/deployment/verify-seaweedfs.sh`

- [ ] **Task 7: Create Bucket Structure for BorgStack Services** (AC: 2, 5)
  - [ ] Create main bucket `borgstack` using S3 API
  - [ ] Create directory structure within bucket:
    - `/borgstack/n8n/` for workflow attachments
    - `/borgstack/chatwoot/avatars/`, `/borgstack/chatwoot/messages/`, `/borgstack/chatwoot/uploads/`
    - `/borgstack/directus/originals/`, `/borgstack/directus/thumbnails/`
    - `/borgstack/fileflows/input/`, `/borgstack/fileflows/output/`
    - `/borgstack/lowcoder/` for app assets
    - `/borgstack/duplicati/` for backup staging
  - [ ] Configure bucket permissions (read/write for authenticated users)
  - [ ] Document bucket structure in `config/seaweedfs/README.md`
  - [ ] Test bucket accessibility using AWS CLI: `aws --endpoint-url http://localhost:8333 s3 ls`

- [ ] **Task 8: Configure Storage Quotas and Limits** (AC: 5)
  - [ ] Set volume size limit via environment variable (Task 2)
  - [ ] Configure max volumes per server (default: 100)
  - [ ] Set up volume pre-allocation for better performance
  - [ ] Configure filer directory quotas (if needed):
    - Use `weed shell` command: `fs.configure -locationPrefix=/buckets/ -volumeGrowthCount=1`
  - [ ] Document quota management in `config/seaweedfs/README.md`
  - [ ] Test quota enforcement by attempting to exceed limits

- [ ] **Task 9: Optional: Configure Caddy Reverse Proxy for S3 API** (AC: 2)
  - [ ] **DECISION POINT:** Determine if S3 API should be exposed externally via Caddy
    - **Option A (Recommended):** Keep S3 on `borgstack_internal` only (services access via internal network)
    - **Option B:** Expose via Caddy for external S3 client access (e.g., backup tools, mobile apps)
  - [ ] If exposing externally:
    - [ ] Add `SEAWEEDFS_HOST=s3.${BORGSTACK_DOMAIN}` to .env.example
    - [ ] Add SeaweedFS to `borgstack_external` network
    - [ ] Add Caddy reverse proxy block for `${SEAWEEDFS_HOST}` in `config/caddy/Caddyfile`
    - [ ] Set proxy target to `http://seaweedfs:8333`
    - [ ] Enable automatic HTTPS via Let's Encrypt
    - [ ] Add security headers and CORS configuration for S3 API
  - [ ] If keeping internal only:
    - [ ] Document internal S3 endpoint: `http://seaweedfs:8333`
    - [ ] Skip Caddy configuration
  - [ ] Document decision in story completion notes

- [ ] **Task 10: Create SeaweedFS User Guide (Portuguese)** (AC: 2, 6)
  - [ ] Create `docs/03-services/seaweedfs.md` documentation file
  - [ ] Add "O que é SeaweedFS?" introduction section
  - [ ] Add "Arquitetura do SeaweedFS" with master/volume/filer/S3 components
  - [ ] Add "Acessando o S3 API" section:
    - Endpoint configuration
    - Credentials setup (access key + secret key)
    - AWS CLI configuration example
    - s3cmd configuration example
  - [ ] Add "Estrutura de Buckets" explaining BorgStack bucket organization
  - [ ] Add "Fazendo Upload de Arquivos" with examples:
    - AWS CLI upload example
    - s3cmd upload example
    - Python boto3 code example
  - [ ] Add "Fazendo Download de Arquivos" with examples
  - [ ] Add "Gerenciamento de Volumes" section:
    - Checking cluster status
    - Growing volumes
    - Monitoring storage usage
  - [ ] Add "Estratégia de Replicação" explaining single vs multi-server
  - [ ] Add "Integração com Serviços" (migration plan to Story 5.3)
  - [ ] Add "Solução de Problemas" troubleshooting section

- [ ] **Task 11: Create S3 Client Configuration Examples** (AC: 2, 6)
  - [ ] Create `config/seaweedfs/aws-cli-config.sh` with AWS CLI setup:
    ```bash
    aws configure set aws_access_key_id ${SEAWEEDFS_ACCESS_KEY}
    aws configure set aws_secret_access_key ${SEAWEEDFS_SECRET_KEY}
    aws configure set default.region us-east-1
    aws configure set default.s3.signature_version s3v4
    ```
  - [ ] Create `config/seaweedfs/s3cmd-config.ini` template
  - [ ] Create `config/seaweedfs/example-upload.sh` demonstrating file upload
  - [ ] Create `config/seaweedfs/example-download.sh` demonstrating file download
  - [ ] Document Python boto3 usage in README.md with code example
  - [ ] Test all examples and verify they work correctly

- [ ] **Task 12: Run Deployment Tests** (AC: 1-6) - **REQUIRES MANUAL EXECUTION DURING DEPLOYMENT**
  - [ ] Execute `tests/deployment/verify-seaweedfs.sh` and confirm all 15 tests pass
  - [ ] Verify no errors in SeaweedFS container logs: `docker compose logs seaweedfs --tail=100`
  - [ ] Verify volumes contain expected directory structure
  - [ ] Test file upload and download using AWS CLI (AC: 6)
  - [ ] Verify cluster status shows healthy: `curl http://localhost:9333/cluster/status`
  - [ ] Document any issues or deviations in story completion notes
  - [ ] **NOTE:** This task requires actual container deployment. Execute: `./tests/deployment/verify-seaweedfs.sh`

- [ ] **Task 13: Update CI Workflow** (AC: 1)
  - [ ] Add `validate-seaweedfs` job to `.github/workflows/ci.yml`
  - [ ] Job should execute `tests/deployment/verify-seaweedfs.sh`
  - [ ] Follow pattern from existing service validation jobs
  - [ ] Ensure job runs after docker-compose validation

## Dev Notes

### Previous Story Context

**Key Learnings from Stories 4.1 and 4.2:**

1. **Storage Migration Pattern (Story 4.1 - Directus):**
   - Directus deployed with local volume storage (`borgstack_directus_uploads`)
   - S3 migration template created: `config/directus/s3-storage.env.example`
   - Migration to SeaweedFS planned for Story 5.3
   - **Critical:** Services must be deployed and tested BEFORE SeaweedFS migration
   [Source: docs/stories/4.1.directus-headless-cms.md]

2. **FileFlows Storage Strategy (Story 4.2):**
   - FileFlows using local volumes: `borgstack_fileflows_input`, `borgstack_fileflows_output`, `borgstack_fileflows_temp`
   - S3 migration template created: `config/fileflows/s3-storage.env.example`
   - Migration planned for Story 5.3 after SeaweedFS deployment
   [Source: docs/stories/4.2.fileflows-media-processing.md]

3. **Migration Sequencing:**
   - **Story 5.1 (Current):** Deploy SeaweedFS with S3 API
   - **Story 5.3 (Future):** Migrate Directus and FileFlows from local volumes to SeaweedFS S3
   - **Why separate stories:** Allows testing SeaweedFS independently before migrating production data

### Architecture Context

**SeaweedFS Component Overview:**
[Source: architecture/components.md#seaweedfs]

SeaweedFS provides S3-compatible object storage for BorgStack's file-heavy services (Directus, FileFlows, Chatwoot, n8n).

**Key Interfaces:**
- **S3 API (port 8333)** - S3-compatible object storage on `borgstack_internal` network
- **Filer API (port 8888)** - File system operations for direct file access
- **Master/Volume server topology** - Master (9333) manages cluster, Volume (8080) stores data

**Dependencies:** None (foundational service)

**Technology Stack:**
- Image: `chrislusf/seaweedfs:3.97`
- Volumes: `seaweedfs_master`, `seaweedfs_volume`, `seaweedfs_filer`
- Configuration: Replication strategy, storage quotas

### SeaweedFS Deployment Architecture

**Single-Server Mode (BorgStack MVP):**
[Source: SeaweedFS documentation + architecture/deployment-architecture.md]

BorgStack uses the **unified server mode** where all SeaweedFS components run in a single container:

```bash
weed server -filer -s3 -ip=<container-ip>
```

This command starts:
1. **Master server** (port 9333) - Volume allocation, topology management
2. **Volume server** (port 8080) - Actual file storage and retrieval
3. **Filer** (port 8888) - File system abstraction layer
4. **S3 API** (port 8333) - S3-compatible HTTP interface

**Why unified mode?**
- Simplifies single-server deployment (one container vs four)
- Reduces resource overhead (shared memory/processes)
- Suitable for servers with < 1TB storage
- Easier to manage and monitor

**Future Multi-Server Expansion:**
When scaling to multiple servers, separate the components:
- Dedicated master containers for cluster coordination
- Multiple volume servers for distributed storage
- Multiple filer instances for HA
- Change replication from `000` to `001` or higher

### S3 API Configuration

**Authentication:**
[Source: SeaweedFS S3 API documentation]

SeaweedFS S3 API uses AWS-compatible authentication:

```yaml
environment:
  AWS_ACCESS_KEY_ID: ${SEAWEEDFS_ACCESS_KEY}
  AWS_SECRET_ACCESS_KEY: ${SEAWEEDFS_SECRET_KEY}
```

**Important Security Notes:**
- Access keys should be randomly generated (32+ characters)
- Secret keys should be 64+ characters
- Store in .env file with 600 permissions
- Never commit credentials to version control
- Use same credentials for all S3 clients (AWS CLI, boto3, s3cmd)

**S3 Endpoint:**
- Internal (services): `http://seaweedfs:8333`
- External (optional, via Caddy): `https://s3.${BORGSTACK_DOMAIN}`

### Volume and Replication Strategy

**Replication Format: `XYZ`**
[Source: SeaweedFS master configuration documentation]

- **X** = Number of copies in different datacenters (0 for single DC)
- **Y** = Number of copies in different racks (0 for single rack)
- **Z** = Number of copies on different servers (0 for single server)

**BorgStack Configuration:**
- **Current (Story 5.1):** `000` = No replication (single server)
- **Future expansion:** `001` = 1 copy on different server (2 total copies)
- **High availability:** `011` = 1 DC copy + 1 rack copy + 1 server copy

**Volume Size Limit:**
Default: 10GB per volume (`SEAWEEDFS_VOLUME_SIZE_LIMIT_MB=10240`)

**Why 10GB volumes?**
- Balances file distribution across volumes
- Enables parallel writes (one write per volume)
- Prevents single volume becoming bottleneck
- Reasonable for media files (videos, images)

**Volume Growth:**
SeaweedFS automatically creates new volumes when existing ones fill up. Manual pre-allocation:
```bash
curl "http://localhost:9333/vol/grow?count=4&replication=000"
```

### Bucket Organization Strategy

**BorgStack Bucket Structure:**
[Source: architecture/database-schema.md#seaweedfs-storage-organization]

```
/buckets/
  borgstack/                    # Main bucket for all services
    n8n/                        # n8n workflow attachments
    chatwoot/                   # Chatwoot conversation files
      avatars/
      messages/
      uploads/
    directus/                   # Directus CMS assets
      originals/
      thumbnails/
      documents/
    fileflows/                  # FileFlows media processing
      input/
      output/
      temp/
    lowcoder/                   # Lowcoder app assets
    duplicati/                  # Backup staging area
```

**Why single bucket with prefixes?**
- Simpler permission management (bucket-level policies)
- Easier to backup (single bucket snapshot)
- Consistent S3 endpoint across services
- Aligns with S3 best practices (use prefixes, not excessive buckets)

### Service Integration Patterns

**How Services Will Use SeaweedFS (Story 5.3):**

**1. Directus CMS:**
```yaml
# .env configuration
STORAGE_LOCATIONS=s3
STORAGE_S3_DRIVER=s3
STORAGE_S3_KEY=${SEAWEEDFS_ACCESS_KEY}
STORAGE_S3_SECRET=${SEAWEEDFS_SECRET_KEY}
STORAGE_S3_BUCKET=borgstack
STORAGE_S3_REGION=us-east-1
STORAGE_S3_ENDPOINT=http://seaweedfs:8333
STORAGE_S3_ROOT=/directus/
```
[Template exists: config/directus/s3-storage.env.example]

**2. FileFlows Media Processing:**
FileFlows doesn't natively support S3 input/output. Integration via n8n:
- n8n downloads from SeaweedFS → copies to FileFlows `/input`
- FileFlows processes → outputs to `/output`
- n8n uploads from `/output` → SeaweedFS
[Template exists: config/fileflows/s3-storage.env.example]

**3. Chatwoot:**
```yaml
ACTIVE_STORAGE_SERVICE=s3
AWS_ACCESS_KEY_ID=${SEAWEEDFS_ACCESS_KEY}
AWS_SECRET_ACCESS_KEY=${SEAWEEDFS_SECRET_KEY}
AWS_REGION=us-east-1
AWS_BUCKET_NAME=borgstack
AWS_S3_ENDPOINT=http://seaweedfs:8333
AWS_S3_PATH_PREFIX=chatwoot/
```

**4. n8n Workflow Attachments:**
n8n HTTP Request nodes support S3 operations using AWS SDK integration.

### Project Structure Alignment

**Configuration Files:**
[Source: architecture/unified-project-structure.md]

```
config/seaweedfs/
├── README.md                  # Setup and usage guide
├── filer.toml                 # Filer configuration template
├── aws-cli-config.sh          # AWS CLI setup script
├── s3cmd-config.ini           # s3cmd configuration template
├── example-upload.sh          # File upload example
└── example-download.sh        # File download example
```

**Documentation:**
```
docs/03-services/seaweedfs.md  # Portuguese user guide
```

**Tests:**
```
tests/deployment/verify-seaweedfs.sh  # Deployment validation test
```

### Performance Considerations

**Storage Performance Expectations:**
[Source: architecture/testing-strategy.md#performance-testing]

- **Disk I/O (SSD):** > 100 MB/s sequential read/write
- **File upload (10MB):** < 2s per file
- **S3 API response time:** p95 < 200ms
- **Volume server throughput:** > 50 concurrent uploads

**Optimization Tips:**
- Use SSD storage for volume directory (`borgstack_seaweedfs_volume`)
- Pre-allocate volumes during low-traffic periods
- Monitor volume distribution (avoid hotspots)
- Enable volume pre-allocation for predictable growth

### Coding Standards Compliance

**Volume Naming:**
[Source: architecture/coding-standards.md#naming-conventions]
- ✅ `borgstack_seaweedfs_master` (follows `borgstack_` prefix)
- ✅ `borgstack_seaweedfs_volume`
- ✅ `borgstack_seaweedfs_filer`

**Network Isolation:**
[Source: architecture/coding-standards.md#critical-infrastructure-rules]
- ✅ SeaweedFS on `borgstack_internal` network only
- ✅ No ports exposed to host (services access via internal network)
- ✅ Optional external access via Caddy reverse proxy (Task 9)

**Configuration as Code:**
- ✅ Filer configuration in `config/seaweedfs/filer.toml`
- ✅ All configuration files in version control
- ✅ Credentials in .env (not committed)

**Health Checks:**
[Source: architecture/coding-standards.md#critical-infrastructure-rules]
- ✅ Health check using master API: `curl -f http://localhost:9333/cluster/status`
- ✅ Interval: 30s, Timeout: 10s, Retries: 5, Start period: 60s

### Testing Strategy

**Testing Philosophy:**
[Source: architecture/testing-strategy.md]

BorgStack focuses on integration and deployment validation, not unit tests (pre-built Docker images).

**Test Coverage Requirements:**

**Integration Tests (Task 6):**
1. Container running and healthy
2. Correct image version (3.97)
3. Master API accessible and responds correctly
4. Volume server API functional
5. Filer API functional
6. S3 API accessible
7. Volume storage writeable
8. Bucket creation via S3 API
9. File upload to S3 (AC: 6)
10. File download from S3 (AC: 6)
11. Replication strategy configured
12. Network isolation verified
13. Storage quotas enforced
14. Cluster status shows healthy
15. No startup errors in logs

**Test Execution:**
```bash
./tests/deployment/verify-seaweedfs.sh

# Expected output:
# ✅ Test 1: SeaweedFS container running
# ✅ Test 2: Correct image version
# ✅ Test 3: Health check passing
# ...
# ✅ Test 15: No startup errors
# PASS: 15/15 tests passed
```

**Manual Testing Requirements (Task 12):**
1. Upload test file using AWS CLI
2. Download file and verify integrity
3. List bucket contents
4. Delete test file
5. Verify storage quotas prevent over-allocation
6. Check cluster status via API

### Environment Variables

**Required Variables (Task 4):**

```bash
# SeaweedFS S3-Compatible Object Storage
SEAWEEDFS_ACCESS_KEY=<generate-random-32-char>
SEAWEEDFS_SECRET_KEY=<generate-random-64-char>
SEAWEEDFS_VOLUME_SIZE_LIMIT_MB=10240  # Max size per volume (default: 10GB)
SEAWEEDFS_REPLICATION=000              # Single server: 000, Multi: 001+
SEAWEEDFS_MAX_VOLUMES=100              # Maximum number of volumes (default: 100)

# Optional: External S3 API access via Caddy
# SEAWEEDFS_HOST=s3.${BORGSTACK_DOMAIN}
```

**Credential Generation:**
```bash
# Generate access key (32 characters)
openssl rand -base64 24

# Generate secret key (64 characters)
openssl rand -base64 48
```

### Known Limitations

**Current Limitations (Story 5.1):**

1. **Single Server Mode:** No replication (`000`). Data loss if server fails.
   - **Mitigation:** Story 5.2 (Duplicati) provides backup solution
   - **Future:** Add second server and change to `001` replication

2. **No Web UI:** SeaweedFS doesn't provide admin UI in default image.
   - **Mitigation:** Use command-line tools (curl, AWS CLI, weed shell)
   - **Future:** Consider deploying SeaweedFS web UI container (optional)

3. **Manual Volume Management:** Volumes must be pre-allocated or grow automatically.
   - **Mitigation:** Monitor volume usage and grow proactively
   - **Future:** Automated volume management script (Story 6.5)

4. **No Object Versioning:** S3 object versioning not enabled by default.
   - **Mitigation:** Duplicati backups provide point-in-time recovery
   - **Future:** Enable versioning in filer configuration

5. **Internal Network Only (Default):** S3 API not exposed externally.
   - **Mitigation:** Task 9 provides optional Caddy reverse proxy
   - **Decision:** Keep internal unless external access required

### Security Considerations

**S3 API Security:**
[Source: architecture/coding-standards.md#critical-infrastructure-rules + SeaweedFS security best practices]

- **Authentication:** AWS signature v4 (access key + secret key)
- **Network Isolation:** `borgstack_internal` network only (default)
- **HTTPS (if exposed):** Caddy automatic SSL via Let's Encrypt
- **Credential Storage:** .env file with 600 permissions
- **Access Control:** Single admin credentials (no per-bucket policies in MVP)

**Future Security Enhancements (Post-MVP):**
- Per-bucket access policies using SeaweedFS identities
- JWT authentication for temporary access
- S3 bucket policies for fine-grained permissions
- Client-side encryption for sensitive data

### Migration Plan to Story 5.3

**Story 5.3 will migrate existing services from local volumes to SeaweedFS:**

1. **Directus CMS Migration:**
   - Copy files from `borgstack_directus_uploads` to SeaweedFS `/borgstack/directus/`
   - Update Directus environment variables (S3 configuration)
   - Restart Directus container
   - Verify assets load correctly
   - Remove local volume (after confirmation)

2. **FileFlows Integration:**
   - Create n8n workflows for S3 download/upload
   - Update Directus-FileFlows integration (Story 4.3)
   - Test end-to-end media processing
   - Remove local volumes (after confirmation)

3. **Chatwoot Migration:**
   - Copy attachments from `borgstack_chatwoot_storage` to SeaweedFS
   - Update Chatwoot environment variables
   - Restart Chatwoot containers
   - Verify attachments accessible

4. **n8n Integration:**
   - No migration needed (n8n uses HTTP Request nodes for S3)
   - Create workflow templates for S3 operations
   - Document S3 integration in n8n guide

**Migration Validation:**
- All existing files accessible after migration
- New uploads go to SeaweedFS (not local volumes)
- No broken image/file links in applications
- Local volumes can be safely removed

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 1.1 | Story validated and approved - all template sections verified, Docker image 3.97 confirmed available, implementation readiness score 10/10 | Product Owner (Sarah) |
| 2025-10-05 | 1.0 | Initial story draft created with comprehensive architecture context and SeaweedFS documentation | Scrum Master (Bob) |

## Dev Agent Record

*This section will be populated by the development agent during implementation*

## QA Results

*This section will be populated by QA agent after story implementation*
