# Story 6.1: Integration Testing Suite

## Status
Draft

## Story
**As a** test engineer,
**I want** comprehensive integration tests for all components,
**so that** I can verify the entire stack works together as expected.

## Acceptance Criteria
1. End-to-end smoke tests for all major workflows
2. Integration tests between all components
3. Basic functionality verified across all services
4. Common failure scenarios tested and documented
5. Test results documented and analyzed

Note: Advanced performance testing, load testing, and security vulnerability assessments are marked as post-MVP enhancements.

## Tasks / Subtasks

- [ ] **Task 0: Review Existing Test Infrastructure** (AC: 2, 3)
  - [ ] Audit all existing test scripts in `tests/deployment/` and `tests/integration/`
  - [ ] Review deployment verification scripts (15 scripts identified)
  - [ ] Review integration test suite from Story 5.3 (storage integration tests)
  - [ ] Document test coverage gaps
  - [ ] Identify which components have no integration tests yet
  - [ ] Create test coverage matrix: Component × Test Type
  - [ ] **CHECKPOINT:** Confirm current test coverage before proceeding

- [ ] **Task 1: Create End-to-End Workflow Test Suite** (AC: 1)
  - [ ] Create `tests/integration/test-e2e-workflows.sh` master test suite
  - [ ] **Setup: Load API credentials from environment**
    - [ ] Source API tokens from .env file (CHATWOOT_API_TOKEN, N8N_API_KEY, EVOLUTION_API_KEY)
    - [ ] Export as environment variables for use in test scripts
    - [ ] Validate required credentials exist before running tests
  - [ ] **Workflow 1 Test: WhatsApp → Chatwoot Customer Service**
    - [ ] Start Evolution API mock webhook server (simulate WhatsApp message)
    - [ ] Verify n8n receives webhook at `/webhook/whatsapp-incoming`
    - [ ] Verify n8n creates/finds Chatwoot contact via API
    - [ ] Verify n8n creates Chatwoot conversation
    - [ ] Verify n8n posts message to Chatwoot
    - [ ] Verify Chatwoot stores message in PostgreSQL (query `chatwoot_db.messages`)
    - [ ] Simulate Chatwoot agent reply via API
    - [ ] Verify n8n webhook receives reply at `/webhook/chatwoot-message-created`
    - [ ] Verify n8n sends message to Evolution API via `/message/sendText`
    - [ ] Cleanup test data (delete conversation, contact)
  - [ ] **Workflow 2 Test: Bootstrap and Deployment**
    - [ ] Verify all 14 containers healthy: `docker compose ps --format json | jq -r '.Health'`
    - [ ] Verify all required volumes exist (15+ volumes)
    - [ ] Verify all networks exist and configured (`borgstack_internal`, `borgstack_external`)
    - [ ] Verify SSL certificates generated (check Caddy data volume)
    - [ ] Verify all services accessible via Caddy reverse proxy
    - [ ] Run all deployment verification scripts (`tests/deployment/verify-*.sh`)
    - [ ] Verify total startup time < 30 minutes (after images pulled)
  - [ ] **Workflow 3 Test: Automated Backup Process**
    - [ ] Trigger Duplicati backup via API: `POST /api/v1/backup/{id}/run`
    - [ ] Verify backup job starts and completes successfully
    - [ ] Verify PostgreSQL dumps created for all databases (n8n_db, chatwoot_db, directus_db, evolution_db)
    - [ ] Verify MongoDB dump created (lowcoder database)
    - [ ] Verify volume snapshots included in backup
    - [ ] Verify backup encryption applied (AES-256)
    - [ ] Verify backup uploaded to configured destination
    - [ ] Verify backup integrity check passes
    - [ ] Document backup completion time and size
  - [ ] **Workflow 4 Test: Media File Processing Pipeline**
    - [ ] Upload test video to Directus via API: `POST /files`
    - [ ] Verify file stored in `borgstack_directus_uploads` volume
    - [ ] Trigger FileFlows processing (manual via API)
    - [ ] Verify FileFlows detects file format (FFprobe)
    - [ ] Verify FileFlows transcodes file (FFmpeg)
    - [ ] Verify processed file created in FileFlows output directory
    - [ ] Verify Directus asset record updated with processed file metadata
    - [ ] Cleanup test files
  - [ ] Make script executable: `chmod +x tests/integration/test-e2e-workflows.sh`
  - [ ] Verify all 4 workflows pass end-to-end

- [ ] **Task 2: Create Component Integration Test Suite** (AC: 2, 3)
  - [ ] Create `tests/integration/test-component-integration.sh`
  - [ ] **Database Integration Tests:**
    - [ ] Test n8n → PostgreSQL connection (query `n8n_db` via n8n API)
    - [ ] Test Chatwoot → PostgreSQL connection (query `chatwoot_db.accounts`)
    - [ ] Test Directus → PostgreSQL connection (query `directus_db.directus_users`)
    - [ ] Test Evolution API → PostgreSQL connection (query `evolution_db.instances`)
    - [ ] Test Lowcoder → MongoDB connection (query `lowcoder` database via API)
    - [ ] Test n8n → Redis connection (verify session storage)
    - [ ] Test Chatwoot → Redis connection (verify Sidekiq jobs)
    - [ ] Test Lowcoder → Redis connection (verify cache)
  - [ ] **Storage Integration Tests:**
    - [ ] Test Directus → SeaweedFS Filer API (upload/download file)
    - [ ] Test FileFlows → SeaweedFS Filer API (read/write media files)
    - [ ] Test n8n → SeaweedFS Filer API (workflow attachments)
  - [ ] **API Integration Tests:**
    - [ ] Test n8n → Evolution API (send WhatsApp message via `/message/sendText`)
    - [ ] Test n8n → Chatwoot API (create contact via `/api/v1/accounts/{id}/contacts`)
    - [ ] Test n8n → Directus API (query assets via `/items/assets`)
    - [ ] Test n8n → FileFlows API (trigger flow via `/api/flow/trigger`)
    - [ ] Test Evolution API → n8n webhook (POST to `/webhook/whatsapp-incoming`)
    - [ ] Test Chatwoot → n8n webhook (POST to `/webhook/chatwoot-message-created`)
    - [ ] Test Directus → n8n webhook (POST to `/webhook/directus-upload`)
    - [ ] Test FileFlows → n8n webhook (POST to `/webhook/fileflows-complete`)
  - [ ] **Reverse Proxy Integration Tests:**
    - [ ] Test Caddy → n8n (HTTPS proxy, verify SSL)
    - [ ] Test Caddy → Chatwoot (verify routing)
    - [ ] Test Caddy → Directus (verify routing)
    - [ ] Test Caddy → Lowcoder (verify routing)
    - [ ] Test Caddy → FileFlows (verify routing)
    - [ ] Test Caddy → Duplicati (verify routing)
    - [ ] Test Caddy → Evolution API (verify routing)
    - [ ] Verify HTTP → HTTPS redirect for all services
  - [ ] **Security Integration Tests:**
    - [ ] Test API authentication: Invalid/missing Chatwoot API token returns 401
    - [ ] Test API authentication: Invalid/missing n8n API key returns 401
    - [ ] Test API authentication: Invalid/missing Evolution API key returns 401
    - [ ] Test CORS headers: Verify Caddy sets proper CORS policies for all services
    - [ ] Test CORS headers: Verify OPTIONS requests handled correctly
  - [ ] Make script executable: `chmod +x tests/integration/test-component-integration.sh`
  - [ ] Verify all integration tests pass (40+ tests)

- [ ] **Task 3: Create Failure Scenario Test Suite** (AC: 4)
  - [ ] Create `tests/integration/test-failure-scenarios.sh`
  - [ ] **Scenario 1: Database Connection Loss**
    - [ ] Stop PostgreSQL container: `docker compose stop postgresql`
    - [ ] Verify n8n fails gracefully (no crash, logs error)
    - [ ] Verify Chatwoot fails gracefully
    - [ ] Restart PostgreSQL: `docker compose start postgresql`
    - [ ] Verify services reconnect automatically within 30s
    - [ ] Verify no data loss (check workflow execution history)
  - [ ] **Scenario 2: Redis Connection Loss**
    - [ ] Stop Redis container: `docker compose stop redis`
    - [ ] Verify Chatwoot Sidekiq fails gracefully (logs error, no crash)
    - [ ] Verify n8n session management degrades gracefully
    - [ ] Restart Redis: `docker compose start redis`
    - [ ] Verify services reconnect automatically
    - [ ] Verify queued jobs resume processing
  - [ ] **Scenario 3: Network Partition (Service Isolation)**
    - [ ] Disconnect n8n from `borgstack_internal`: `docker network disconnect borgstack_internal borgstack_n8n`
    - [ ] Verify n8n cannot reach PostgreSQL (connection timeout)
    - [ ] Verify n8n cannot reach Redis
    - [ ] Reconnect network: `docker network connect borgstack_internal borgstack_n8n`
    - [ ] Verify n8n recovers and reconnects
  - [ ] **Scenario 4: Disk Space Exhaustion**
    - [ ] Simulate low disk space (create large file to fill partition to 95%)
    - [ ] Verify PostgreSQL logs warning about disk space
    - [ ] Verify SeaweedFS rejects uploads (disk full)
    - [ ] Verify Duplicati backup fails with clear error message
    - [ ] Clean up large file, restore disk space
    - [ ] Verify services resume normal operation
  - [ ] **Scenario 5: Service Restart Under Load**
    - [ ] Start background load: 10 concurrent n8n webhook requests
    - [ ] Restart n8n: `docker compose restart n8n`
    - [ ] Verify in-flight requests fail gracefully (503 or timeout)
    - [ ] Verify service recovers within 30s
    - [ ] Verify new requests succeed after recovery
    - [ ] Stop background load
  - [ ] **Scenario 6: Invalid Configuration (Environment Variable)**
    - [ ] Change critical env var to invalid value (e.g., `POSTGRES_PASSWORD=wrong`)
    - [ ] Restart affected service: `docker compose up -d postgresql`
    - [ ] Verify service fails health check
    - [ ] Verify dependent services fail gracefully (log error, don't crash)
    - [ ] Restore correct configuration
    - [ ] Verify services recover
  - [ ] Make script executable: `chmod +x tests/integration/test-failure-scenarios.sh`
  - [ ] Document all failure scenarios and expected behavior in `docs/05-troubleshooting.md`
  - [ ] Verify all 6 failure scenarios handled correctly

- [ ] **Task 4: Create Test Execution and Reporting Framework** (AC: 5)
  - [ ] Create `tests/run-all-tests.sh` master test runner
  - [ ] Execute all deployment verification tests sequentially
  - [ ] Execute all integration tests sequentially
  - [ ] Execute all failure scenario tests sequentially
  - [ ] Generate test report in JSON format: `test-results.json`
  - [ ] Generate test report in Markdown format: `docs/qa/test-results-$(date +%Y%m%d).md`
  - [ ] Include in report:
    - [ ] Total tests executed
    - [ ] Tests passed / failed / skipped
    - [ ] Execution time per test suite
    - [ ] Failure details with logs for failed tests
    - [ ] System metrics during test execution (CPU, memory, disk)
  - [ ] Create test coverage matrix showing all components tested
  - [ ] Document test execution instructions in `tests/README.md`
  - [ ] Make master test runner executable: `chmod +x tests/run-all-tests.sh`

- [ ] **Task 5: Add Integration Tests to CI/CD Pipeline** (AC: 2, 3)
  - [ ] Add `integration-tests` job to `.github/workflows/ci.yml`
  - [ ] Configure job to run on:
    - [ ] Push to `main` branch
    - [ ] Pull request creation/update
    - [ ] Manual workflow dispatch
  - [ ] Job steps:
    - [ ] Checkout code
    - [ ] Set up Docker and Docker Compose
    - [ ] Load .env.example as .env (with test credentials)
    - [ ] Run `docker compose up -d`
    - [ ] Wait for all services healthy (max 10 minutes)
    - [ ] Execute deployment verification tests
    - [ ] Execute component integration tests
    - [ ] Execute end-to-end workflow tests (excluding failure scenarios for CI)
    - [ ] Generate test report artifact
    - [ ] Upload test results as GitHub Actions artifact
    - [ ] Tear down: `docker compose down -v`
  - [ ] Configure test timeout: 30 minutes
  - [ ] Verify CI job passes on current `main` branch

- [ ] **Task 6: Document Test Results and Analysis** (AC: 5)
  - [ ] Execute full test suite: `./tests/run-all-tests.sh`
  - [ ] Analyze test results and identify any issues
  - [ ] Create comprehensive test report: `docs/qa/integration-test-report.md`
  - [ ] Include in report:
    - [ ] Executive Summary (total coverage, pass rate)
    - [ ] Test Suite Breakdown (deployment, integration, e2e, failure scenarios)
    - [ ] Component Coverage Matrix
    - [ ] Workflow Coverage Analysis
    - [ ] Performance Baselines (test execution time, system metrics)
    - [ ] Known Issues and Limitations
    - [ ] Recommendations for future test improvements
  - [ ] Document testing methodology and standards in `docs/08-testing.md`
  - [ ] Create troubleshooting guide based on failure scenario tests

- [ ] **Task 7: Create Continuous Testing Documentation** (AC: 1, 2, 3, 4, 5)
  - [ ] Document test execution schedule (when to run tests)
  - [ ] Document how to add new integration tests
  - [ ] Document test data management (setup/teardown)
  - [ ] Document test environment requirements
  - [ ] Create developer guide for writing integration tests
  - [ ] Document test best practices and patterns
  - [ ] Add testing section to main README.md with quick start

## Dev Notes

### Previous Story Insights

**From Story 5.3 (Storage Integration Testing):**
[Source: docs/stories/5.3.storage-integration-testing.story.md#dev-agent-record]

- **Testing Philosophy**: Focus on deployment validation and integration verification (not unit tests)
- **Integration Test Pattern**: Bash scripts in `tests/integration/test-*.sh` with clear pass/fail output
- **CI/CD Pattern**: Add validation jobs to `.github/workflows/ci.yml` for automated checks
- **Test Script Template**: Use standard bash test patterns with proper error handling
- **Test Independence**: Each test can run standalone, proper cleanup/teardown
- **QA Gate Process**: CONCERNS → fixes → PASS (comprehensive review required)

### Architecture Context

**Testing Strategy:**
[Source: docs/architecture/testing-strategy.md]

**Testing Philosophy:**
- **No unit tests**: Services are pre-built Docker images; upstream maintains their own tests
- **Focus on integration**: Verify services communicate correctly
- **Deployment validation**: Ensure clean deployment succeeds
- **Configuration verification**: Validate docker-compose.yml and .env correctness

**Testing Pyramid:**
```
                   E2E/Integration Tests
                   /                    \
          Deployment Validation    Service Integration
          /                                          \
   Configuration Tests                        API Connectivity Tests
```

**Performance Baselines (36GB RAM, 8 vCPU Server):**

| Metric | Target (p95) | Acceptable (p99) | Critical Threshold |
|--------|--------------|------------------|--------------------|
| **API Response Time** | < 200ms | < 500ms | > 1000ms |
| **Webhook Throughput** | 100 req/s | 50 req/s | < 25 req/s |
| **Database Connections** | < 150 concurrent | < 180 concurrent | > 200 (pool exhausted) |
| **Redis Operations** | > 10,000 ops/s | > 5,000 ops/s | < 1,000 ops/s |
| **Disk I/O (SSD)** | > 100 MB/s | > 50 MB/s | < 20 MB/s |
| **Memory Usage** | < 75% | < 85% | > 90% (swap risk) |
| **CPU Usage** | < 70% avg | < 85% avg | > 95% sustained |

### Core Workflows to Test

**Workflow 1: WhatsApp to Chatwoot Customer Service Integration**
[Source: docs/architecture/core-workflows.md#workflow-1]

**Key Integration Points:**
- Evolution API receives WhatsApp message → webhooks to n8n at `/webhook/whatsapp-incoming`
- n8n queries Chatwoot API to find/create contact: `GET /api/v1/accounts/{id}/contacts`
- n8n creates conversation: `POST /api/v1/accounts/{id}/conversations`
- n8n posts message to Chatwoot: `POST /api/v1/accounts/{id}/conversations/{id}/messages`
- Chatwoot stores in PostgreSQL `chatwoot_db.messages` table
- Chatwoot queues notification in Redis (Sidekiq)
- Agent replies via Chatwoot UI → webhooks to n8n at `/webhook/chatwoot-message-created`
- n8n sends reply to Evolution API: `POST /message/sendText`
- Evolution API delivers message to WhatsApp

**Error Handling Scenarios to Test:**
1. Chatwoot API returns 500 error → n8n retries 3x with exponential backoff
2. Contact creation race condition → n8n catches 422 error, re-queries contact
3. Evolution API webhook delivery failure → Evolution retries 5x, n8n has backup poll mechanism
4. WhatsApp rate limit (429) → n8n waits per Retry-After header, retries once
5. Redis connection loss → Sidekiq jobs queued, processed when Redis recovers

**Workflow 2: Initial Deployment and Bootstrap**
[Source: docs/architecture/core-workflows.md#workflow-2]

**Deployment Steps:**
1. System preparation: apt updates, Docker install
2. Environment variable generation: `.env` from `.env.example`
3. Validation: DNS, disk space (500GB), RAM (16GB minimum)
4. Image pull: All 14 service images (~15GB)
5. Service startup: Infrastructure first (PostgreSQL, MongoDB, Redis, SeaweedFS), then applications
6. Caddy SSL: Request Let's Encrypt certificates (HTTP-01 challenge)
7. Health check validation: All services respond with 200 OK
8. Total time: 4-6 hours per NFR1 requirement

**Workflow 3: Automated Backup Process**
[Source: docs/architecture/core-workflows.md#workflow-3]

**Backup Components:**
- PostgreSQL dumps: n8n_db, chatwoot_db, directus_db, evolution_db (pg_dump)
- MongoDB dump: lowcoder database (mongodump)
- Volume snapshots: postgresql_data, redis_data, seaweedfs_*, n8n_data, evolution_instances, caddy_data
- Compression: zstd algorithm
- Encryption: AES-256 with passphrase
- Upload: To external storage (S3/B2/FTP)
- Retention: 7 daily, 4 weekly, 12 monthly, 5 yearly (configurable)
- Estimated size: ~50GB full backup
- Network transfer time: 30-120 minutes (bandwidth dependent)

**Workflow 4: Media File Processing Pipeline**
[Source: docs/architecture/core-workflows.md#workflow-4]

**Processing Flow:**
1. User uploads video to Directus via CMS UI
2. Directus stores file in `borgstack_directus_uploads` volume
3. Directus saves asset metadata in PostgreSQL `directus_db.assets` table
4. Directus triggers n8n webhook: `POST /webhook/directus-upload` with file metadata
5. n8n triggers FileFlows: `POST /api/flow/trigger` with filename and source path
6. FileFlows reads file from Directus upload volume
7. FileFlows detects format using FFprobe
8. FileFlows transcodes using FFmpeg: `-c:v libx264 -crf 23 -preset medium -c:a aac`
9. FileFlows writes processed file to output directory
10. FileFlows webhooks n8n: `POST /webhook/fileflows-complete` with processed file metadata
11. n8n updates Directus asset: `PATCH /items/assets/{id}` with processed URL
12. Directus updates PostgreSQL record

### Component Specifications

**All Services to Test:**
[Source: docker compose ps output, docs/architecture/components.md]

**Infrastructure Services (5):**
1. **PostgreSQL with pgvector** - Port 5432 (internal only), 4 databases (n8n_db, chatwoot_db, directus_db, evolution_db)
2. **MongoDB** - Port 27017 (internal only), 1 database (lowcoder)
3. **Redis** - Port 6379 (internal only), shared cache/queue
4. **SeaweedFS** - Ports 9333 (master), 8080 (volume), 8888 (filer), 8333 (S3 API) - all internal
5. **Caddy** - Ports 80/443 (external), reverse proxy with automatic SSL

**Application Services (9):**
6. **n8n** - Workflow automation hub, depends on PostgreSQL + Redis
7. **Evolution API** - WhatsApp integration, depends on PostgreSQL
8. **Chatwoot** - Customer service, depends on PostgreSQL + Redis
9. **Lowcoder (3 containers)** - Low-code platform:
   - lowcoder-api-service (depends on MongoDB + Redis)
   - lowcoder-node-service (depends on API service)
   - lowcoder-frontend (depends on API service)
10. **Directus** - Headless CMS, depends on PostgreSQL + Redis
11. **FileFlows** - Media processing, no database dependencies
12. **Duplicati** - Backup system, no database dependencies

**Total Containers: 14**

### API Endpoints to Test

**n8n Webhooks:**
[Source: docs/architecture/core-workflows.md]
- `POST /webhook/whatsapp-incoming` - Receive WhatsApp messages from Evolution API
- `POST /webhook/chatwoot-message-created` - Receive Chatwoot agent replies
- `POST /webhook/directus-upload` - Receive Directus file upload events
- `POST /webhook/fileflows-complete` - Receive FileFlows processing completion
- `POST /webhook/fileflows-error` - Receive FileFlows processing errors

**Chatwoot API Endpoints:**
[Source: docs/architecture/core-workflows.md]
- `GET /api/v1/accounts/{id}/contacts` - Search contacts by phone number
- `POST /api/v1/accounts/{id}/contacts` - Create new contact
- `POST /api/v1/accounts/{id}/conversations` - Create or get conversation
- `POST /api/v1/accounts/{id}/conversations/{id}/messages` - Add message to conversation

**Evolution API Endpoints:**
[Source: docs/architecture/core-workflows.md]
- `POST /message/sendText` - Send WhatsApp text message
- `GET /message/list` - List messages (for backup sync)

**Directus API Endpoints:**
[Source: docs/architecture/core-workflows.md]
- `POST /files` - Upload file to CMS
- `GET /items/assets` - Query assets
- `PATCH /items/assets/{id}` - Update asset metadata

**FileFlows API Endpoints:**
[Source: docs/architecture/core-workflows.md]
- `POST /api/flow/trigger` - Trigger media processing workflow

**Duplicati API Endpoints:**
[Source: docs/architecture/core-workflows.md]
- `POST /api/v1/backup/{id}/run` - Trigger backup job manually

**SeaweedFS Filer API:**
[Source: docs/stories/5.3.storage-integration-testing.story.md]
- `POST /buckets/{bucket}/{path}` - Upload file
- `GET /buckets/{bucket}/{path}` - Download file
- `GET /buckets/{bucket}/` - List directory
- `DELETE /buckets/{bucket}/{path}` - Delete file

### File Locations

**Test Scripts:**
[Source: docs/architecture/unified-project-structure.md]
```
tests/
├── integration/                           # Integration test scripts
│   ├── test-e2e-workflows.sh              # End-to-end workflow tests (NEW)
│   ├── test-component-integration.sh      # Component integration tests (NEW)
│   ├── test-failure-scenarios.sh          # Failure scenario tests (NEW)
│   ├── test-storage-integration.sh        # Storage tests (existing from Story 5.3)
│   └── README.md                          # Test execution guide (NEW)
├── deployment/                            # Deployment validation (15 existing scripts)
│   ├── verify-postgresql.sh               # PostgreSQL deployment verification
│   ├── verify-mongodb.sh                  # MongoDB deployment verification
│   ├── verify-redis.sh                    # Redis deployment verification
│   ├── verify-seaweedfs.sh                # SeaweedFS deployment verification
│   ├── verify-caddy.sh                    # Caddy deployment verification
│   ├── verify-n8n.sh                      # n8n deployment verification
│   ├── verify-evolution.sh                # Evolution API deployment verification
│   ├── verify-chatwoot.sh                 # Chatwoot deployment verification
│   ├── verify-lowcoder.sh                 # Lowcoder deployment verification
│   ├── verify-directus.sh                 # Directus deployment verification
│   ├── verify-directus-fileflows.sh       # Directus-FileFlows integration
│   ├── verify-fileflows.sh                # FileFlows deployment verification
│   ├── verify-duplicati.sh                # Duplicati deployment verification
│   ├── verify-network-isolation.sh        # Network security verification
│   └── verify-bootstrap.sh                # Bootstrap script verification
└── run-all-tests.sh                       # Master test runner (NEW)
```

**Test Results and Documentation:**
```
docs/
├── qa/
│   ├── test-results-YYYYMMDD.md           # Test execution reports (NEW)
│   └── integration-test-report.md         # Comprehensive test analysis (NEW)
├── 05-troubleshooting.md                  # Troubleshooting guide (UPDATE with failure scenarios)
└── 08-testing.md                          # Testing methodology and standards (NEW)
```

**CI/CD:**
```
.github/workflows/ci.yml                   # Add integration-tests job (UPDATE)
```

### Testing Requirements

**Test Script Standards:**
[Source: docs/architecture/testing-strategy.md, docs/stories/5.3.storage-integration-testing.story.md]

**Bash Testing Framework:**
- Use standard bash test patterns
- Each test outputs PASS/FAIL with descriptive messages
- Exit code 0 = all tests pass, Exit code 1 = at least one test failed
- Test independence (each test can run standalone)
- Proper cleanup/teardown to avoid side effects

**Integration Test Pattern:**
```bash
#!/bin/bash
# tests/integration/test-example.sh

set -e

echo "========================================"
echo "Example Integration Tests"
echo "========================================"

# Test 1: Example test
if docker compose ps example | grep -q "healthy"; then
  echo "✅ Test 1/N: Example test... PASS"
else
  echo "❌ Test 1/N: Example test... FAIL"
  exit 1
fi

# Test 2: Another test
RESULT=$(curl -s -f http://localhost:8080/health)
if [ "$RESULT" = "OK" ]; then
  echo "✅ Test 2/N: Another test... PASS"
else
  echo "❌ Test 2/N: Another test... FAIL"
  exit 1
fi

echo ""
echo "========================================"
echo "All tests passed!"
echo "========================================"
```

**Test Execution Order:**
1. Deployment verification tests (validate all services running)
2. Component integration tests (validate service-to-service communication)
3. End-to-end workflow tests (validate complete user journeys)
4. Failure scenario tests (validate error handling and resilience)

**Test Data Management:**
- Use test-specific data that can be safely created and deleted
- Clean up all test data after test execution
- Use predictable test identifiers (e.g., `test-contact-12345`)
- Document test data requirements and cleanup procedures

**CI/CD Integration:**
- Run deployment verification and component integration tests on every commit
- Run end-to-end workflow tests on PR and main branch
- Skip failure scenario tests in CI (destructive, run manually)
- Generate test reports as GitHub Actions artifacts
- Fail CI build if any critical tests fail

### Coding Standards Compliance

**Test Script Naming:**
[Source: docs/architecture/coding-standards.md#naming-conventions]
- Use kebab-case: `test-e2e-workflows.sh`, `test-component-integration.sh`
- Prefix with `test-` for integration tests
- Prefix with `verify-` for deployment validation tests

**Shell Script Best Practices:**
[Source: docs/architecture/coding-standards.md]
- Use `#!/bin/bash` shebang
- Enable strict mode: `set -e` (exit on error)
- Use descriptive variable names in SCREAMING_SNAKE_CASE
- Comment complex logic
- Make scripts executable: `chmod +x tests/integration/*.sh`

**Health Check Requirements:**
[Source: docs/architecture/coding-standards.md#critical-infrastructure-rules]
- All services must have health checks defined in docker-compose.yml
- Health checks must return quickly (< 5s timeout)
- Use `docker compose ps --format json | jq -r '.Health'` to verify health status

### Project Structure Alignment

**No Conflicts Detected:**

All file paths align with defined project structure:
- Integration tests in `tests/integration/`
- Deployment verification in `tests/deployment/`
- Test documentation in `docs/qa/` and `docs/08-testing.md`
- CI/CD in `.github/workflows/ci.yml`
- Test runner script in `tests/run-all-tests.sh`

## Testing

### Test File Locations
[Source: docs/architecture/unified-project-structure.md]

```
tests/
├── integration/
│   ├── test-e2e-workflows.sh              # End-to-end workflow tests (4 workflows)
│   ├── test-component-integration.sh      # Component integration tests (40+ tests)
│   ├── test-failure-scenarios.sh          # Failure scenario tests (6 scenarios)
│   └── README.md                          # Test execution guide
├── deployment/                            # 15 existing deployment verification scripts
└── run-all-tests.sh                       # Master test runner
```

### Test Standards
[Source: docs/architecture/testing-strategy.md]

**Testing Philosophy:**
- Focus on deployment validation and integration verification
- No unit tests (pre-built Docker images from upstream)
- Verify all acceptance criteria through automated tests
- Establish performance baselines for capacity planning

**Test Execution:**
```bash
# Run all tests
./tests/run-all-tests.sh

# Run specific test suite
./tests/integration/test-e2e-workflows.sh
./tests/integration/test-component-integration.sh
./tests/deployment/verify-*.sh

# Run with verbose output
bash -x ./tests/integration/test-e2e-workflows.sh
```

**Expected Output:**
```
========================================
BorgStack Integration Test Suite
========================================

Running Deployment Verification Tests...
✅ verify-postgresql.sh: PASS
✅ verify-mongodb.sh: PASS
✅ verify-redis.sh: PASS
...

Running Component Integration Tests...
✅ Test 1/35: n8n → PostgreSQL connection... PASS
✅ Test 2/35: Chatwoot → PostgreSQL connection... PASS
...

Running End-to-End Workflow Tests...
✅ Workflow 1: WhatsApp → Chatwoot... PASS
✅ Workflow 2: Bootstrap and Deployment... PASS
✅ Workflow 3: Automated Backup... PASS
✅ Workflow 4: Media Processing... PASS

========================================
Test Summary
========================================
Total Tests: 60
Passed: 60
Failed: 0
Execution Time: 15 minutes

All tests passed! Stack is production-ready.
========================================
```

### Testing Frameworks and Patterns
[Source: docs/architecture/testing-strategy.md]

**Bash Testing Framework:**
- Use standard bash test patterns with clear output
- Exit code 0 = all tests pass, Exit code 1 = at least one test failed
- Test independence (each test can run standalone)
- Proper cleanup/teardown to avoid test data pollution

**Performance Testing Tools:**
- **wrk**: HTTP load testing for API endpoints (`sudo apt install wrk`)
- **ab (Apache Bench)**: Simple HTTP throughput testing (`sudo apt install apache2-utils`)
- **pgbench**: PostgreSQL performance benchmarking (included with PostgreSQL)
- **redis-benchmark**: Redis performance testing (included with Redis)
- **iostat**: Disk I/O performance monitoring (`sudo apt install sysstat`)

**Test Coverage Matrix Template:**

| Component | Deployment | Integration | E2E Workflow | Failure Scenario |
|-----------|------------|-------------|--------------|------------------|
| PostgreSQL | ✅ | ✅ | ✅ | ✅ |
| MongoDB | ✅ | ✅ | ✅ | ⬜ |
| Redis | ✅ | ✅ | ✅ | ✅ |
| SeaweedFS | ✅ | ✅ | ⬜ | ⬜ |
| Caddy | ✅ | ✅ | ✅ | ⬜ |
| n8n | ✅ | ✅ | ✅ | ✅ |
| Evolution API | ✅ | ✅ | ✅ | ✅ |
| Chatwoot | ✅ | ✅ | ✅ | ✅ |
| Lowcoder | ✅ | ✅ | ⬜ | ⬜ |
| Directus | ✅ | ✅ | ✅ | ⬜ |
| FileFlows | ✅ | ✅ | ✅ | ⬜ |
| Duplicati | ✅ | ✅ | ✅ | ✅ |

### Specific Testing Requirements for This Story

**AC: 1 - End-to-End Workflow Tests:**
- Test all 4 core workflows defined in architecture
- Verify data flows correctly through entire workflow
- Verify error handling in workflow execution
- Document execution time and resource usage

**AC: 2 - Component Integration Tests:**
- Test all service-to-service integrations (40+ tests including security)
- Verify database connections for all services
- Verify API connectivity between components
- Verify webhook delivery and processing

**AC: 3 - Basic Functionality Verification:**
- Run all 15 deployment verification scripts
- Verify all 14 containers healthy
- Verify all volumes and networks configured correctly
- Verify SSL certificates generated

**AC: 4 - Failure Scenario Tests:**
- Test 6 common failure scenarios
- Verify graceful degradation (no crashes)
- Verify automatic recovery mechanisms
- Document expected behavior for each scenario

**AC: 5 - Test Results Documentation:**
- Generate comprehensive test report
- Create test coverage matrix
- Document performance baselines
- Provide recommendations for improvements

**CI/CD Integration:**
- Add `integration-tests` job to `.github/workflows/ci.yml`
- Run tests on every commit to main and PR
- Upload test results as GitHub Actions artifacts
- Fail build if critical tests fail

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-07 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-10-07 | 1.1 | Added API credential setup in Task 1, added security integration tests (auth validation, CORS) in Task 2, updated test count to 40+ | Sarah (Product Owner) |

## Dev Agent Record

*This section will be populated by the development agent during implementation.*

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results

*This section will be populated by the QA agent during review.*
