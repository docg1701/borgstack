# Story 4.3: Directus-FileFlows Integration

## Status
Approved

## Story
**As an** integration specialist,
**I want** Directus to utilize FileFlows for media processing,
**so that** content management includes automated media optimization.

## Acceptance Criteria
1. Directus can trigger FileFlows workflows
2. Media files automatically processed on upload
3. Processing results reflected in Directus
4. Error handling for failed processes
5. Performance monitoring for media operations
6. Storage optimization through automated processing

## Tasks / Subtasks

- [ ] **Task 0: Verify Prerequisites and Configure n8n Volume Access** (AC: 1)
  - [ ] Verify Directus container is running and healthy: `docker compose ps directus | grep "healthy"`
  - [ ] Verify FileFlows container is running and healthy: `docker compose ps fileflows | grep "healthy"`
  - [ ] Verify n8n container is running and healthy: `docker compose ps n8n | grep "healthy"`
  - [ ] Verify Directus is accessible via Caddy: `curl -I https://${DIRECTUS_HOST}/server/health`
  - [ ] Verify FileFlows is accessible via Caddy: `curl -I https://${FILEFLOWS_HOST}/`
  - [ ] Verify n8n is accessible: `curl -I https://${N8N_HOST}/`
  - [ ] **CRITICAL: Configure n8n volume mounts for file access:**
    - Add to n8n service in docker-compose.yml:
      ```yaml
      volumes:
        - borgstack_directus_uploads:/directus/uploads:ro  # Read-only access
        - borgstack_fileflows_input:/fileflows/input:rw     # Read-write access
      ```
    - Restart n8n: `docker compose up -d n8n`
    - Verify mounts: `docker compose exec n8n ls -la /directus/uploads /fileflows/input`
  - [ ] If any prerequisite is missing, HALT and notify user to complete prerequisite stories first

- [ ] **Task 1: Create n8n Workflow - Directus to FileFlows File Copy** (AC: 1, 2)
  - [ ] Access n8n UI at `https://${N8N_HOST}`
  - [ ] Create new workflow: "Directus → FileFlows Media Processing"
  - [ ] Add Webhook Trigger node: `POST /webhook/directus-upload`
  - [ ] Configure webhook to accept Directus Flow events (file upload notifications)
  - [ ] Add Function node to extract file metadata (filename, mimetype, size, directus_file_id)
  - [ ] Add IF node to filter only media files (video/*, audio/*, image/*)
  - [ ] Add Execute Command node to copy file from Directus uploads to FileFlows input:
    - Command: `cp /directus/uploads/{{$json["payload"]["filename_disk"]}} /fileflows/input/{{$json["payload"]["filename_download"]}}`
    - Note: Uses mounted volumes configured in Task 0
    - Alternative if copy fails: Use `docker compose exec` from host (requires n8n access to Docker socket)
  - [ ] Add Function node to generate processing tracking ID (UUID) for this file
  - [ ] Add HTTP Request node to store processing status in n8n database (pending)
  - [ ] Add Error Handling node for copy failures (retry 3 times with exponential backoff)
  - [ ] Test workflow with sample Directus Flow event
  - [ ] Save and activate workflow
  - [ ] NOTE: FileFlows will auto-detect new files in `/input` and start processing based on configured Flows

- [ ] **Task 2: Configure Directus Flow for File Upload Notification** (AC: 1, 2)
  - [ ] Access Directus admin panel at `https://${DIRECTUS_HOST}`
  - [ ] Navigate to Settings → Flows (Directus Automate)
  - [ ] Create new Flow: "FileFlows Processing Trigger"
  - [ ] Configure Flow Trigger:
    - Type: Event Hook
    - Action Type: Action (Non-Blocking)
    - Scope: `files.upload`
    - Collections: `directus_files`
  - [ ] Add Condition Operation to filter media files:
    - Condition: `$trigger.payload.type` contains `video/` OR `audio/` OR `image/`
    - Rule: `{"$trigger": {"payload": {"type": {"_contains": "video/"}}}` (create 3 conditions with OR)
  - [ ] Add Webhook/Request URL Operation:
    - Method: POST
    - URL: `https://${N8N_HOST}/webhook/directus-upload`
    - Request Body: `{"key": "{{$trigger.key}}", "collection": "{{$trigger.collection}}", "payload": {{$trigger.payload}}}`
    - Headers (Optional - for webhook security):
      - `X-Webhook-Signature: {{$env.DIRECTUS_WEBHOOK_SECRET}}` (enable if webhook authentication configured)
  - [ ] (Optional) Enable webhook signature validation for security:
    - In Directus Flow, add custom header with HMAC signature
    - In n8n webhook, add validation node to verify signature
    - Store shared secret in environment variable
  - [ ] Save and activate Flow
  - [ ] Test Flow by uploading media file to Directus
  - [ ] Verify n8n webhook receives event in execution log

- [ ] **Task 3: Create n8n Workflow - FileFlows Completion Handler** (AC: 3, 4)
  - [ ] Create new workflow: "FileFlows → Directus Update Results"
  - [ ] Add Webhook Trigger node: `POST /webhook/fileflows-complete`
  - [ ] Configure webhook to accept FileFlows webhook payloads (structure TBD - will be defined when configuring FileFlows webhook in Task 5)
  - [ ] Add Function node to extract: original_filename, processed_filename, output_path, processing_status
  - [ ] Add HTTP Request node to query Directus API:
    - URL: `https://${DIRECTUS_HOST}/items/directus_files`
    - Method: GET
    - Query params: `filter[filename_download][_eq]={{original_filename}}`
    - Headers: `Authorization: Bearer ${DIRECTUS_API_TOKEN}`
  - [ ] Add HTTP Request node to update Directus file record:
    - URL: `https://${DIRECTUS_HOST}/items/directus_files/{{file_id}}`
    - Method: PATCH
    - Headers: `Authorization: Bearer ${DIRECTUS_API_TOKEN}`
    - Body: `{"processed_url": "{{processed_url}}", "processing_status": "completed", "processing_metadata": {{metadata}}}`
  - [ ] Add IF node to check processing status (success vs failure)
  - [ ] For failures: Add Email/Notification node to alert admin
  - [ ] Test workflow with manual webhook call (simulate FileFlows completion)
  - [ ] Save and activate workflow

- [ ] **Task 4: Configure Directus Custom Fields for Processed Media** (AC: 3)
  - [ ] Access Directus admin panel → Settings → Data Model
  - [ ] Navigate to `directus_files` collection
  - [ ] Add custom field: `processed_url` (type: String, interface: Input)
  - [ ] Add custom field: `processing_status` (type: String, interface: Dropdown, options: "pending", "processing", "completed", "failed")
  - [ ] Add custom field: `processing_metadata` (type: JSON, interface: Input Code)
  - [ ] Set default value for `processing_status`: "pending"
  - [ ] Update Directus file detail view to display new fields
  - [ ] Document field schema in config/directus/README.md

- [ ] **Task 5: Configure FileFlows Webhooks for Completion Notifications** (AC: 3, 4)
  - [ ] Access FileFlows admin panel at `https://${FILEFLOWS_HOST}`
  - [ ] Navigate to Settings → Webhooks
  - [ ] Create webhook: "Processing Complete Notification"
  - [ ] Set webhook URL: `https://${N8N_HOST}/webhook/fileflows-complete`
  - [ ] Select trigger: "Flow Execution Complete"
  - [ ] Configure payload: Include `original_filename`, `processed_filename`, `output_path`, `metadata`, `status`, `error_message` (if failed)
  - [ ] Set HTTP method: POST
  - [ ] Add retry configuration: 3 retries with 5s, 10s, 20s backoff
  - [ ] Test webhook by manually running a flow
  - [ ] Verify webhook received in n8n execution log

- [ ] **Task 6: Create FileFlows Processing Flows for Common Media Types** (AC: 2, 6)
  - [ ] Create flow: "Video Optimization H.264"
    - Input: Any video format
    - Processing: Transcode to H.264 with CRF 23, AAC audio, medium preset
    - Output: `/processed/video/{filename}.mp4`
  - [ ] Create flow: "Audio Normalization"
    - Input: Any audio format
    - Processing: Normalize audio levels, convert to MP3 320kbps
    - Output: `/processed/audio/{filename}.mp3`
  - [ ] Create flow: "Image WebP Conversion"
    - Input: JPEG, PNG images
    - Processing: Convert to WebP, resize to max 1920x1080, quality 85%
    - Output: `/processed/images/{filename}.webp`
  - [ ] Set flows to trigger automatically via API
  - [ ] Document flows in config/fileflows/README.md
  - [ ] Export flows to config/fileflows/directus-integration-flows.json

- [ ] **Task 7: Implement Error Handling and Monitoring** (AC: 4, 5)
  - [ ] Create n8n workflow: "FileFlows Error Handler"
  - [ ] Add Webhook Trigger node: `POST /webhook/fileflows-error`
  - [ ] Add Function node to parse error details and categorize (timeout, invalid format, storage failure, etc.)
  - [ ] Add HTTP Request node to update Directus file: Set `processing_status` to "failed", add error details to `processing_metadata`
  - [ ] Add Email Send node to notify admin of processing failures
  - [ ] Add logging node to store error in n8n execution data
  - [ ] Configure FileFlows to send error webhooks to n8n
  - [ ] Test error handling by uploading unsupported file format
  - [ ] Verify error appears in Directus and admin receives notification

- [ ] **Task 8: Create Performance Monitoring Dashboard in n8n** (AC: 5)
  - [ ] Create n8n workflow: "Media Processing Stats Collector"
  - [ ] Add Schedule Trigger node: Run every 15 minutes
  - [ ] Add HTTP Request node to query FileFlows API: `GET /api/statistics`
  - [ ] Extract metrics: Total processed files, average processing time, success rate, queue depth
  - [ ] Add HTTP Request node to query Directus API: Count files by `processing_status`
  - [ ] Add Function node to calculate metrics: Files pending, processing time trends, error rate
  - [ ] Add HTTP Request node to send metrics to monitoring endpoint (optional: custom logging service or email report)
  - [ ] Document metrics in config/n8n/workflows/README.md
  - [ ] Save and activate workflow

- [ ] **Task 9: Create Integration Documentation** (AC: 1, 2, 3, 4, 5, 6)
  - [ ] Create `docs/04-integrations/directus-fileflows.md` (Portuguese)
  - [ ] Document integration architecture (sequence diagram in Mermaid)
  - [ ] Document Directus webhook configuration steps
  - [ ] Document FileFlows flow creation process
  - [ ] Document n8n workflow setup with screenshots
  - [ ] Add troubleshooting guide (common errors and solutions)
  - [ ] Add performance tuning tips (batch processing, queue management)
  - [ ] Add storage optimization strategies (cleanup policies, compression settings)
  - [ ] Document how to monitor integration health
  - [ ] Add example use cases (automatic video transcoding, image optimization, audio podcast processing)

- [ ] **Task 10: Create Deployment Verification Tests** (AC: 1, 2, 3, 4, 5, 6)
  - [ ] Create `tests/integration/test-directus-fileflows.sh`
  - [ ] Test 1: Verify n8n workflows exist and are active
  - [ ] Test 2: Verify Directus webhook is configured correctly
  - [ ] Test 3: Upload test image to Directus, verify FileFlows processes it
  - [ ] Test 4: Verify processed file appears in Directus with updated metadata
  - [ ] Test 5: Test error handling by uploading invalid file format
  - [ ] Test 6: Verify error notification is sent to admin
  - [ ] Test 7: Verify FileFlows webhooks are received by n8n
  - [ ] Test 8: Check performance monitoring workflow collects metrics
  - [ ] Test 9: Verify storage cleanup (original files removed after processing)
  - [ ] Test 10: End-to-end test (upload → process → update → verify)
  - [ ] Make script executable: `chmod +x tests/integration/test-directus-fileflows.sh`
  - [ ] Document test execution in integration guide

- [ ] **Task 11: Update .env.example and Storage Optimization Configuration** (AC: 6)
  - [ ] Add required variable to .env.example:
    - `DIRECTUS_API_TOKEN=<generate-static-token>` with note "Generate in Directus: Settings → Access Tokens"
  - [ ] Add optional variables to .env.example:
    - `FILEFLOWS_DELETE_ORIGINALS=false` with comment "Delete original files after processing (true/false)"
    - `DIRECTUS_MEDIA_RETENTION_DAYS=30` with comment "Days to keep original media files before cleanup"
  - [ ] Configure FileFlows to delete original files after successful processing (optional, configurable via env var)
  - [ ] Add n8n workflow step to optionally delete raw uploads from Directus after processing
  - [ ] Configure retention policy: Keep processed files, optionally delete originals after 30 days
  - [ ] Document storage optimization settings in docs/04-integrations/directus-fileflows.md
  - [ ] Document how to generate DIRECTUS_API_TOKEN in integration guide

## Dev Notes

### Integration Architecture Overview

Story 4.3 implements automated media processing integration between Directus CMS and FileFlows using n8n as the orchestration layer. This follows the **Media File Processing Pipeline** pattern defined in the architecture.

[Source: architecture/core-workflows.md#workflow-4-media-file-processing-pipeline]

### Integration Pattern

The integration uses a **webhook-based event-driven architecture**:

1. **Upload Trigger**: User uploads media file to Directus CMS
2. **Webhook to n8n**: Directus fires webhook to n8n on file upload
3. **Processing Trigger**: n8n calls FileFlows API to start processing
4. **File Processing**: FileFlows processes media file (transcode, optimize, convert)
5. **Completion Webhook**: FileFlows notifies n8n when processing completes
6. **Result Update**: n8n updates Directus file record with processed file URL and metadata

[Source: architecture/core-workflows.md#workflow-4-media-file-processing-pipeline]

### Previous Story Context

**Story 4.1 (Directus CMS):**
- Directus deployed with PostgreSQL (directus_db) + Redis caching
- Local volume storage: `borgstack_directus_uploads`
- S3 migration template prepared for Story 5.1 SeaweedFS integration
- Webhook capabilities available in Directus admin panel
- Admin user and role management configured

**Story 4.2 (FileFlows Media Processing):**
- FileFlows deployed with 5 volumes: data, logs, temp, input, output
- Local volume storage until Story 5.1 SeaweedFS migration
- API endpoint: `/api/flow/trigger` for programmatic flow execution
- Example flows created in `config/fileflows/example-flows.json`:
  - Video H.264 transcode
  - Audio normalization
  - Image WebP conversion
- Processing node configured for local server
- Webhook capabilities for completion/error notifications

### Service Endpoints

**Directus API:**
- Base URL: `https://${DIRECTUS_HOST}`
- Health: `/server/health`
- Files API: `/items/directus_files`
- Webhooks: Settings → Webhooks in admin panel
- Authentication: Bearer token (from `DIRECTUS_ADMIN_EMAIL` / `DIRECTUS_ADMIN_PASSWORD`)

**FileFlows API:**
- Base URL: `https://${FILEFLOWS_HOST}`
- Flow Trigger: `/api/flow/trigger` (POST)
- Statistics: `/api/statistics` (GET)
- Authentication: Session-based (configured in FileFlows admin panel)

**n8n Workflow Platform:**
- Base URL: `https://${N8N_HOST}`
- Webhook endpoints: `/webhook/{webhook-name}` (POST)
- Example webhooks:
  - `/webhook/directus-upload` - Receives Directus file upload events
  - `/webhook/fileflows-complete` - Receives FileFlows completion events
  - `/webhook/fileflows-error` - Receives FileFlows error events
- Authentication: n8n admin credentials or API key

[Source: architecture/backend-architecture.md + epic-details.md Stories 4.1, 4.2]

### n8n Workflow Integration Pattern

BorgStack uses **n8n as the central integration hub** for connecting services via HTTP/webhook patterns. Services do not communicate directly; all integration flows through n8n workflows.

[Source: architecture/backend-architecture.md]

**n8n Workflow Nodes:**
- **Webhook Trigger**: Receives events from Directus/FileFlows
- **HTTP Request**: Calls external service APIs (Directus REST API, FileFlows API)
- **Function**: Data transformation and mapping between services
- **IF**: Conditional logic for filtering and routing
- **Error Trigger**: Catch and handle API failures
- **Schedule Trigger**: Periodic monitoring and cleanup tasks

### Data Models and Payload Examples

**IMPORTANT:** The exact payload structures below are **proposed based on Directus Flow patterns and FileFlows webhook capabilities**. These will be verified and adjusted during implementation.

**Directus Flow Event Payload (Proposed):**
Based on Directus documentation patterns for Event Hook triggers:
```json
{
  "event": "files.upload",
  "key": "abc123-uuid",
  "collection": "directus_files",
  "payload": {
    "id": "abc123-uuid",
    "filename_download": "sample-video.mp4",
    "type": "video/mp4",
    "filesize": 15728640,
    "storage": "local",
    "uploaded_on": "2025-10-05T12:00:00Z"
  }
}
```
[Based on: Directus Flow Event Hook trigger patterns]

**File Transfer Mechanism:**
FileFlows does NOT expose a REST API for triggering flows. Integration works by:
1. Copying file to FileFlows monitored `/input` directory
2. FileFlows auto-detects new files and processes them based on configured Flows
3. FileFlows sends webhook when processing completes (optional - configurable in FileFlows)

**FileFlows Completion Webhook Payload (Proposed):**
FileFlows webhook structure to be determined during Task 5 when configuring webhook in FileFlows UI. Expected fields based on FileFlows documentation:
```json
{
  "original_filename": "sample-video.mp4",
  "processed_filename": "sample-video-optimized.mp4",
  "output_path": "/output/sample-video-optimized.mp4",
  "status": "completed",
  "metadata": {
    "processing_time": 45,
    "codec": "h264",
    "resolution": "1920x1080"
  }
}
```
[Structure TBD - will be defined when configuring FileFlows webhook]

**Directus File Update Request:**
```json
{
  "description": "Processed by FileFlows on 2025-10-05",
  "processed_url": "{{output_file_url}}",
  "processing_status": "completed",
  "processing_metadata": {
    "original_size": 15728640,
    "processing_time": 45,
    "processed_by": "fileflows"
  }
}
```
[Based on: Directus REST API PATCH /items/directus_files/{id}]

### Error Handling Strategy

**Error Scenarios and Responses:**

[Source: architecture/core-workflows.md#error-handling-scenarios]

**Scenario 1: FileFlows Processing Timeout**
- **Detection**: FileFlows processing exceeds 30 minutes without completion webhook
- **Response**:
  1. n8n scheduled workflow checks processing status every 5 minutes
  2. Query FileFlows API for job status
  3. If stuck: Cancel job via FileFlows API, mark Directus file as "failed"
  4. Send alert email to admin with file details
  5. Log error in n8n execution data

**Scenario 2: Invalid File Format**
- **Detection**: FileFlows returns error "Unsupported file format"
- **Response**:
  1. n8n receives error webhook from FileFlows
  2. Update Directus file `processing_status` to "failed"
  3. Add error message to `processing_metadata`: "Unsupported format"
  4. Send notification to user who uploaded file (optional)
  5. Do not retry (permanent failure)

**Scenario 3: Storage Full**
- **Detection**: FileFlows cannot write output file due to disk space
- **Response**:
  1. FileFlows returns error "Insufficient storage"
  2. n8n workflow catches error, triggers storage cleanup workflow
  3. Delete old processed files from `/output` (older than retention period)
  4. Retry processing once after cleanup
  5. If still fails: Alert admin to provision more storage

**Scenario 4: Directus API Failure (500)**
- **Detection**: Directus API returns 500 Internal Server Error when n8n tries to update file record
- **Response**:
  1. n8n workflow retry logic: 3 attempts with exponential backoff (5s, 10s, 20s)
  2. If all retries fail: Store update payload in n8n database for manual replay
  3. Send alert email to admin
  4. Continue workflow execution (don't block other files)
  5. Manual recovery: Admin reviews failed updates and replays via n8n UI

**Scenario 5: Webhook Delivery Failure**
- **Detection**: Directus webhook to n8n fails (n8n temporarily down)
- **Response**:
  1. Directus retries webhook delivery (built-in retry mechanism)
  2. If all retries fail: File uploaded but not processed
  3. n8n scheduled "Missed Files Detector" workflow runs every 30 minutes:
     - Query Directus for files with `processing_status` = "pending" older than 10 minutes
     - Manually trigger FileFlows processing for missed files
  4. Update file status to "processing"

### Storage Strategy

**Current State (Story 4.3):**
- Directus: Local volume `borgstack_directus_uploads` mounted at `/directus/uploads`
- FileFlows: Local volumes `borgstack_fileflows_input` (mounted at `/input`), `borgstack_fileflows_output` (mounted at `/output`)
- **File Transfer Mechanism:**
  - n8n container mounts both Directus and FileFlows volumes
  - n8n copies files between volumes using Execute Command node: `cp /directus/uploads/{file} /fileflows/input/{file}`
  - FileFlows auto-detects new files in `/input` directory and triggers processing
  - No direct API call to FileFlows - integration via filesystem monitoring

**Future State (Story 5.1 SeaweedFS Migration):**
- Directus: S3-compatible storage via SeaweedFS
- FileFlows: S3-compatible storage via SeaweedFS
- File transfer: Both services access shared SeaweedFS storage (no copying needed)
- Storage optimization: Single file stored in SeaweedFS, both services reference same object via S3 URLs

[Source: docs/stories/4.1.directus-headless-cms.md + docs/stories/4.2.fileflows-media-processing.md + FileFlows directory monitoring architecture]

### Project Structure Alignment

**Integration Documentation Location:**
```
docs/04-integrations/directus-fileflows.md
```
[Source: architecture/unified-project-structure.md]

**n8n Workflow Configuration:**
```
config/n8n/workflows/
├── directus-fileflows-upload.json
├── directus-fileflows-complete.json
├── directus-fileflows-error.json
└── README.md
```

**FileFlows Integration Flows:**
```
config/fileflows/directus-integration-flows.json
```

**Integration Tests:**
```
tests/integration/test-directus-fileflows.sh
```
[Source: architecture/unified-project-structure.md]

### Performance Considerations

**Media Processing Performance:**
- Video transcoding: ~0.5x realtime (2 minute video = 4 minutes processing)
- Image optimization: ~1-2 seconds per image
- Audio normalization: ~0.3x realtime

**Queue Management:**
- FileFlows queue depth: Monitor to prevent backlog
- Recommended: Max 10 concurrent processing jobs (based on 8 vCPU server)
- If queue exceeds 20 files: Alert admin to scale processing capacity

**Storage Optimization:**
- Delete original files after successful processing (configurable)
- Compression ratio: Video ~60% smaller, Images ~40% smaller (WebP)
- Expected savings: 40-50% total storage reduction

[Source: architecture/testing-strategy.md#performance-testing]

### Coding Standards Compliance

**Volume Naming:**
- ✅ All volumes use `borgstack_` prefix
- ✅ Descriptive names: `borgstack_directus_uploads`, `borgstack_fileflows_output`

**Network Configuration:**
- ✅ Services on `borgstack_internal` network
- ✅ No direct database port exposure

**Configuration as Code:**
- ✅ n8n workflows exported to `config/n8n/workflows/`
- ✅ FileFlows flows exported to `config/fileflows/directus-integration-flows.json`
- ✅ Integration documentation in version control

[Source: architecture/coding-standards.md]

### Testing

#### Testing Strategy
[Source: architecture/testing-strategy.md]

BorgStack testing focuses on **integration verification** rather than unit testing, since all services are pre-built Docker images.

**Testing Philosophy:**
- No unit tests: Services are pre-built containers
- Focus on integration: Verify services communicate correctly
- End-to-end validation: Test complete workflows from upload to processed result

#### Test File Location
```
tests/integration/test-directus-fileflows.sh
```
[Source: architecture/unified-project-structure.md]

#### Test Coverage Requirements

**Integration Tests:**
1. **Webhook Connectivity**: Verify Directus can trigger n8n webhooks
2. **API Integration**: Verify n8n can call FileFlows API successfully
3. **File Processing**: Upload test file, verify FileFlows processes it
4. **Result Update**: Verify Directus file record updated with processed metadata
5. **Error Handling**: Test invalid file format, verify error workflow triggers
6. **Performance**: Measure processing time for sample files
7. **Storage**: Verify files in correct directories after processing
8. **Cleanup**: Verify optional original file deletion works
9. **Monitoring**: Verify metrics collection workflow runs successfully
10. **End-to-End**: Complete flow from upload to Directus update

**Test Execution:**
```bash
# Run integration test suite
./tests/integration/test-directus-fileflows.sh

# Expected output:
# ✅ Test 1: n8n workflows active
# ✅ Test 2: Directus webhook configured
# ✅ Test 3: File processing triggered
# ✅ Test 4: Directus updated with results
# ✅ Test 5: Error handling works
# ...
# PASS: 10/10 tests passed
```

#### Manual Testing Requirements

**Manual Verification Steps:**
1. Upload video file to Directus CMS
2. Monitor FileFlows UI to see processing job appear
3. Wait for processing to complete
4. Check Directus file record for updated `processed_url` and metadata
5. Verify processed file accessible and optimized
6. Test error case: Upload invalid file format, verify error notification

### Environment Variables

**Existing Variables (from previous stories):**

**Directus:**
- `DIRECTUS_HOST` - Directus domain (configured in Story 4.1)
- `DIRECTUS_ADMIN_EMAIL` - Admin authentication
- `DIRECTUS_ADMIN_PASSWORD` - Admin authentication

**FileFlows:**
- `FILEFLOWS_HOST` - FileFlows domain (configured in Story 4.2)

**n8n:**
- `N8N_HOST` - n8n domain (configured in Story 2.1)

**New Variables Required (added in this story):**

- `DIRECTUS_API_TOKEN` - Static API token for n8n to authenticate with Directus API
  - Generate in Directus: Settings → Access Tokens → Create Token
  - Permissions: Read/Write access to `directus_files` collection
  - Used by n8n to query and update Directus file records

**Optional Variables (for enhanced features):**

- `FILEFLOWS_DELETE_ORIGINALS=false` - Delete source files after processing (default: false)
- `DIRECTUS_MEDIA_RETENTION_DAYS=30` - Retention period for raw uploads before cleanup (default: 30 days)

[Source: Story 4.1, 4.2 environment configurations + Directus API authentication requirements]

### Integration Workflow Sequence

**Detailed Step-by-Step Flow:**

1. **User uploads video.mp4 to Directus CMS** (via Directus admin UI)
2. **Directus creates file record** in `directus_files` collection with `processing_status: "pending"`
3. **Directus Flow triggers** (Event Hook on `files.upload` event for `directus_files` collection)
4. **Directus Flow filters** media files (Condition: `$trigger.payload.type` contains `video/`, `audio/`, or `image/`)
5. **Directus Flow sends webhook** to `https://${N8N_HOST}/webhook/directus-upload` with file metadata
6. **n8n receives webhook**, extracts file details (filename_disk, filename_download, type, size, id)
7. **n8n filters by media type** (IF node: only process video/*, audio/*, image/*)
8. **n8n copies file** from Directus uploads to FileFlows input:
   - Execute Command: `cp /directus/uploads/{filename_disk} /fileflows/input/{filename_download}`
   - Uses mounted volumes configured in docker-compose.yml
9. **FileFlows auto-detects new file** in `/input` directory (library monitoring - every 60 seconds)
10. **FileFlows matches file to Flow** (based on file extension and Flow configuration)
11. **FileFlows processes file**: Transcode to H.264, optimize bitrate, resize if needed
12. **FileFlows saves processed file** to `/output/{filename}-optimized.mp4`
13. **FileFlows sends completion webhook** to `https://${N8N_HOST}/webhook/fileflows-complete` with processing results (optional - if webhook configured)
14. **n8n receives completion event**, extracts processed file info
15. **n8n queries Directus API**: `GET /items/directus_files?filter[filename_download][_eq]={filename}`
16. **n8n updates Directus file record**: `PATCH /items/directus_files/{id}` with `{processed_url, processing_status: "completed", processing_metadata}`
17. **Directus saves updated record**, admin can now see processed file in CMS
18. **Optional: n8n cleanup workflow** deletes original file from `/input` directory (if `FILEFLOWS_DELETE_ORIGINALS=true`)

[Based on: Directus Flows event hooks + FileFlows directory monitoring + n8n workflow orchestration]

### Known Limitations

**Current Limitations (to be addressed in future stories):**

1. **Local Storage Only**: Files stored in local Docker volumes. SeaweedFS S3 migration in Story 5.1 will enable distributed storage and eliminate file copying between services.

2. **Single Processing Node**: FileFlows runs on single server. Horizontal scaling not configured (future enhancement).

3. **No Automatic Retry for Failed Jobs**: Failed processing jobs require manual intervention. Automatic retry mechanism could be added in future iteration.

4. **Basic Monitoring**: Performance metrics collected but no alerting thresholds configured. Advanced monitoring (Grafana/Prometheus) is post-MVP enhancement.

5. **Sequential Processing**: Files processed one at a time in queue. Batch processing optimization possible in future.

### Security Considerations

**Webhook Security:**
- n8n webhook endpoints are HTTPS-only (enforced by Caddy)
- Consider adding webhook signature validation (Directus supports HMAC signatures)
- n8n authentication required for workflow access

**File Access:**
- Directus files accessible only to authenticated users
- FileFlows processing runs in isolated container
- No direct file system access between services (mediated by n8n)

**API Authentication:**
- Directus API calls use Bearer token authentication
- FileFlows API uses session-based authentication
- Store credentials in n8n encrypted credential storage

[Source: architecture/coding-standards.md#critical-infrastructure-rules]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-05 | 1.0 | Initial story draft created | Scrum Master (Bob) |

## Dev Agent Record

### Agent Model Used
_To be populated by Dev Agent during implementation_

### Debug Log References
_To be populated by Dev Agent during implementation_

### Completion Notes List
_To be populated by Dev Agent during implementation_

### File List
_To be populated by Dev Agent during implementation_

## QA Results
_To be populated by QA Agent after review_
