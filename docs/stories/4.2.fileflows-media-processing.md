# Story 4.2: FileFlows Media Processing

## Status
Draft

## Story
**As a** media specialist,
**I want** FileFlows 25.09 deployed with storage integration,
**so that** I can automate media processing workflows.

## Acceptance Criteria
1. FileFlows container running with specified version
2. Connection to SeaweedFS for storage working
3. Media processing workflows configured
4. Input/output directories properly mapped
5. Processing nodes and libraries initialized
6. Basic media conversion flows tested

## Tasks / Subtasks

- [ ] **Task 0: Verify Prerequisite Services and SeaweedFS Dependency Resolution** (AC: 2)
  - [ ] Verify Caddy container is running and healthy: `docker compose ps caddy | grep "healthy"`
  - [ ] Verify `config/caddy/Caddyfile` exists and is configured from Story 1.5
  - [ ] Check if SeaweedFS is deployed (Story 5.1): `docker compose ps seaweedfs 2>/dev/null || echo "NOT_DEPLOYED"`
  - [ ] **CRITICAL DECISION:** If SeaweedFS NOT deployed, implement local volume storage strategy (similar to Story 4.1 Directus approach)
  - [ ] Document storage strategy decision in story completion notes
  - [ ] If using local storage: Create migration plan and template for Story 5.1 SeaweedFS integration

- [ ] **Task 1: Add FileFlows Service to docker-compose.yml** (AC: 1, 2, 4)
  - [ ] Add FileFlows service definition using `revenz/fileflows:25.09` image
  - [ ] Configure FileFlows on both `borgstack_internal` and `borgstack_external` networks
  - [ ] Mount persistent volumes:
    - `borgstack_fileflows_data` for application data and configuration
    - `borgstack_fileflows_logs` for processing logs
    - `borgstack_fileflows_temp` for temporary processing files
    - `borgstack_fileflows_input` for input media files (local storage until Story 5.1)
    - `borgstack_fileflows_output` for processed media files (local storage until Story 5.1)
  - [ ] Set environment variables for FileFlows configuration
  - [ ] Add restart policy: `unless-stopped`
  - [ ] Configure container name: `fileflows`
  - [ ] Add dependency on Caddy (condition: service_started) for reverse proxy availability

- [ ] **Task 2: Configure FileFlows Environment Variables** (AC: 1, 3)
  - [ ] Set TZ (timezone) from .env variable `${TZ:-America/Sao_Paulo}`
  - [ ] Set PUID and PGID for file permissions (default: 1000:1000)
  - [ ] Set FileFlows base URL to `https://${FILEFLOWS_HOST}`
  - [ ] Configure temp directory path: `/temp`
  - [ ] Configure log level: `Information` (default) or from `${FILEFLOWS_LOG_LEVEL}`
  - [ ] Add license key variable if commercial features needed: `${FILEFLOWS_LICENSE_KEY}` (optional)

- [ ] **Task 3: Configure FileFlows Local Storage (Temporary until Story 5.1)** (AC: 2, 4)
  - [ ] Set input directory: `/input` (mapped to borgstack_fileflows_input volume)
  - [ ] Set output directory: `/output` (mapped to borgstack_fileflows_output volume)
  - [ ] Set temp directory: `/temp` (mapped to borgstack_fileflows_temp volume)
  - [ ] Add TODO comment in docker-compose.yml: "MIGRATION: Switch to SeaweedFS S3 storage in Story 5.1"
  - [ ] Document S3 migration plan in config/fileflows/README.md under "Future Enhancements" section
  - [ ] Prepare S3 configuration template in config/fileflows/s3-storage.env.example for Story 5.1:
    - STORAGE_TYPE=s3
    - S3_ENDPOINT=http://seaweedfs:8333
    - S3_BUCKET=borgstack
    - S3_INPUT_PREFIX=fileflows/input/
    - S3_OUTPUT_PREFIX=fileflows/output/
    - S3_ACCESS_KEY=${SEAWEEDFS_ACCESS_KEY}
    - S3_SECRET_KEY=${SEAWEEDFS_SECRET_KEY}

- [ ] **Task 4: Implement FileFlows Health Check** (AC: 1)
  - [ ] Add health check using `curl -f http://localhost:5000/ || exit 1`
  - [ ] Set health check interval to 30 seconds
  - [ ] Set timeout to 10 seconds
  - [ ] Set retries to 5 before marking unhealthy
  - [ ] Set start_period to 60 seconds (application initialization)

- [ ] **Task 5: Update .env.example with FileFlows Variables** (AC: 1, 3)
  - [ ] Add `FILEFLOWS_HOST=fileflows.${BORGSTACK_DOMAIN}` variable
  - [ ] Add `FILEFLOWS_LOG_LEVEL=Information` with comment "Options: Trace, Debug, Information, Warning, Error, Critical"
  - [ ] Add `TZ=America/Sao_Paulo` (timezone) if not already present
  - [ ] Add `PUID=1000` and `PGID=1000` for file permission management
  - [ ] Add comment section: "FileFlows Media Processing Configuration"
  - [ ] Add note: "File storage uses local volumes initially - S3 migration in Story 5.1"
  - [ ] Add optional: `FILEFLOWS_LICENSE_KEY=<optional-for-commercial-features>` (commented out)

- [ ] **Task 6: Configure FileFlows Caddy Reverse Proxy** (AC: 1)
  - [ ] Add Caddy reverse proxy block for `${FILEFLOWS_HOST}` in `config/caddy/Caddyfile`
  - [ ] Set proxy target to `http://fileflows:5000`
  - [ ] Enable automatic HTTPS via Let's Encrypt
  - [ ] Configure security headers (X-Content-Type-Options, X-Frame-Options)
  - [ ] Set appropriate timeout for long-running processing requests (proxy_timeout 3600s)
  - [ ] Add WebSocket support for real-time processing status updates

- [ ] **Task 7: Create FileFlows Configuration Directory** (AC: 3, 5)
  - [ ] Create `config/fileflows/` directory
  - [ ] Create `config/fileflows/README.md` with setup instructions
  - [ ] Document how to access FileFlows web UI
  - [ ] Document initial setup wizard steps (admin account, processing nodes)
  - [ ] Document library configuration process
  - [ ] Document flow creation basics (input → processing → output)
  - [ ] Add "Future Enhancements" section documenting S3 migration planned for Story 5.1
  - [ ] Create example flow configuration in `config/fileflows/example-flows.json`:
    - Video H.264 transcoding flow
    - Audio normalization flow
    - Image WebP conversion flow

- [ ] **Task 8: Create FileFlows Deployment Verification Test** (AC: 1, 2, 3, 4, 5)
  - [ ] Create `tests/deployment/verify-fileflows.sh` test script
  - [ ] Test 1: Verify FileFlows container is running: `docker compose ps fileflows | grep -q "Up"`
  - [ ] Test 2: Verify correct image version: `docker compose ps fileflows | grep -q "revenz/fileflows:25.09"`
  - [ ] Test 3: Verify health check is passing: `docker compose ps fileflows | grep -q "healthy"`
  - [ ] Test 4: Verify local storage volumes are mounted and accessible
  - [ ] Test 5: Verify web UI accessibility: `curl -f https://${FILEFLOWS_HOST}/`
  - [ ] Test 6: Verify input directory is writable from container
  - [ ] Test 7: Verify output directory is writable from container
  - [ ] Test 8: Verify temp directory is writable from container
  - [ ] Test 9: Verify networks configured (borgstack_internal + borgstack_external)
  - [ ] Test 10: Verify Caddy reverse proxy routing works
  - [ ] Test 11: Verify HTTPS certificate is valid via Caddy
  - [ ] Test 12: Check FileFlows logs for startup errors
  - [ ] Make script executable: `chmod +x tests/deployment/verify-fileflows.sh`

- [ ] **Task 9: Configure Initial FileFlows Processing Nodes** (AC: 5)
  - [ ] Document processing node setup in config/fileflows/README.md
  - [ ] Create processing node configuration for local server
  - [ ] Set node capacity based on server resources (CPU cores, RAM)
  - [ ] Configure temp directory for processing: `/temp`
  - [ ] Set concurrent processing limit (default: 2 for 8 vCPU server)
  - [ ] Enable FFmpeg processing capabilities
  - [ ] Document hardware transcoding options (if GPU available)

- [ ] **Task 10: Create Basic Media Processing Flows** (AC: 3, 6)
  - [ ] Create video transcoding flow (AC: 6):
    - Input: /input directory (monitor for new video files)
    - Process: FFmpeg H.264 encode (libx264, CRF 23, preset medium)
    - Output: /output directory with transcoded file
  - [ ] Create audio normalization flow:
    - Input: /input directory (monitor for audio files)
    - Process: FFmpeg loudnorm filter
    - Output: /output directory with normalized audio
  - [ ] Create image optimization flow:
    - Input: /input directory (monitor for image files)
    - Process: FFmpeg WebP conversion with quality 80
    - Output: /output directory with optimized images
  - [ ] Document each flow configuration in config/fileflows/README.md
  - [ ] Export flow definitions to config/fileflows/example-flows.json

- [ ] **Task 11: Create FileFlows User Guide (Portuguese)** (AC: 3, 5)
  - [ ] Create `docs/03-services/fileflows.md` documentation file
  - [ ] Add "O que é FileFlows?" introduction section
  - [ ] Add "Acessando o FileFlows" section with login instructions
  - [ ] Add "Configurando Nós de Processamento" section with setup guide
  - [ ] Add "Criando Bibliotecas" section for input/output library setup
  - [ ] Add "Criando Fluxos de Processamento" section with flow builder guide
  - [ ] Add "Exemplos de Fluxos Comuns" with pre-configured flow templates
  - [ ] Add "Integração com Directus e n8n" examples for automation
  - [ ] Add "Monitoramento de Processamento" section for job tracking
  - [ ] Add "Solução de Problemas" troubleshooting section

- [ ] **Task 12: Test FileFlows Media Processing** (AC: 6)
  - [ ] Copy test video file to input directory: `docker compose cp test.mp4 fileflows:/input/`
  - [ ] Trigger video transcoding flow manually via FileFlows UI
  - [ ] Monitor processing progress in FileFlows dashboard
  - [ ] Verify transcoded output appears in /output directory
  - [ ] Verify output file plays correctly and meets quality expectations
  - [ ] Test audio normalization flow with sample audio file
  - [ ] Test image optimization flow with sample image file
  - [ ] Document processing times and resource usage
  - [ ] Verify error handling for corrupted/unsupported files
  - [ ] Document test results in story completion notes

- [ ] **Task 13: Run All Deployment Tests** (AC: 1-6)
  - [ ] Execute `tests/deployment/verify-fileflows.sh` and confirm all tests pass
  - [ ] Verify no errors in FileFlows container logs: `docker compose logs fileflows --tail=100`
  - [ ] Verify Caddy routing shows successful requests to fileflows:5000
  - [ ] Verify local storage volumes contain expected directory structure
  - [ ] Document any issues or deviations in story completion notes

## Dev Notes

### Previous Story Context

**Key learnings from Story 4.1 (Directus Headless CMS):**

1. **Storage Strategy:** Story 4.1 used local volume storage initially (borgstack_directus_uploads) with migration plan to SeaweedFS in Story 5.1
2. **Prerequisite Verification:** Verified PostgreSQL, Redis, Caddy, and bootstrap.sh before implementation (Task 0)
3. **S3 Migration Template:** Created s3-storage.env.example for future SeaweedFS migration
4. **Test Coverage:** Comprehensive 26-test suite including API validation tests
5. **Documentation:** Both technical (README.md) and user guides (Portuguese) created

**Critical Pattern for Story 4.2:**
⚠️ **STORAGE DEPENDENCY RESOLUTION:** AC2 requires "Connection to SeaweedFS for storage working" but Story 5.1 (SeaweedFS deployment) hasn't been completed yet. Following Story 4.1's approach:
- Use local volumes (borgstack_fileflows_input, borgstack_fileflows_output, borgstack_fileflows_temp) for initial implementation
- Create S3/SeaweedFS migration template (config/fileflows/s3-storage.env.example) for Story 5.1
- Document clear migration path in README.md

### Architecture Context

**Service Purpose:**
FileFlows is an automated media file conversion and processing platform that watches input directories, applies transformation workflows, and outputs processed media.
[Source: architecture/components.md#fileflows-media-processing]

**Deployment Model:** Single container with FFmpeg-based processing engine
[Source: architecture/components.md#fileflows-media-processing]

### Component Details

**Container Specifications:**
[Source: architecture/components.md#fileflows-media-processing]
- Image: `revenz/fileflows:25.09`
- Exposed Port: 5000 (Web UI)
- Networks: `borgstack_internal`, `borgstack_external`
- Dependencies: SeaweedFS (file storage) - **Story 5.1 NOT YET IMPLEMENTED**

**Key Interfaces:**
- Web UI (port 5000) exposed via Caddy reverse proxy
- File processing engine watching input directories
- Output to SeaweedFS (Story 5.1) or local storage (Story 4.2 initial implementation)

**Technology Stack:**
- Processing Engine: FFmpeg for video/audio/image transcoding
- Workflow Builder: Visual flow designer for processing pipelines
- Node.js runtime for application logic

### Local Storage Configuration (Temporary until Story 5.1)

**Current Implementation (Story 4.2):**

FileFlows will use local Docker volumes for initial deployment:

```yaml
volumes:
  borgstack_fileflows_data:    # Application data and configuration
  borgstack_fileflows_logs:    # Processing logs
  borgstack_fileflows_temp:    # Temporary processing workspace
  borgstack_fileflows_input:   # Input media files (watch directory)
  borgstack_fileflows_output:  # Processed output files
```

**Volume Mounts:**
- `/config` → borgstack_fileflows_data (persistent configuration)
- `/logs` → borgstack_fileflows_logs (processing history)
- `/temp` → borgstack_fileflows_temp (scratch space for transcoding)
- `/input` → borgstack_fileflows_input (media input directory)
- `/output` → borgstack_fileflows_output (processed media output)

### Future SeaweedFS S3 Storage Configuration (Story 5.1)

**Planned Migration Path:**
[Source: architecture/database-schema.md#seaweedfs-storage-organization]

After Story 5.1 (SeaweedFS deployment) is complete, FileFlows will migrate to S3-compatible storage:

```
SeaweedFS Bucket Structure:
/borgstack/fileflows/
  ├── input/       # Input media files
  ├── output/      # Processed media files
  └── temp/        # Temporary processing files
```

**Planned S3 Environment Variables (Story 5.1):**
- Storage Type: `s3`
- S3 Endpoint: `http://seaweedfs:8333`
- S3 Bucket: `borgstack`
- Input Prefix: `fileflows/input/`
- Output Prefix: `fileflows/output/`
- S3 Credentials: `${SEAWEEDFS_ACCESS_KEY}`, `${SEAWEEDFS_SECRET_KEY}`

**Migration Template:**
A configuration template will be created in `config/fileflows/s3-storage.env.example` for easy transition when Story 5.1 is complete.

### Media Processing Workflow Integration

**Workflow 4: Media File Processing Pipeline**
[Source: architecture/core-workflows.md#workflow-4-media-file-processing-pipeline]

FileFlows integrates with Directus and n8n for automated media processing:

1. **User uploads video** to Directus CMS
2. **Directus stores** file to SeaweedFS (Story 5.1) or local storage (Story 4.2)
3. **Directus triggers** n8n webhook with file metadata
4. **n8n calls** FileFlows API to start processing flow
5. **FileFlows downloads** source file from storage
6. **FileFlows processes** file (transcode H.264, normalize audio, optimize image)
7. **FileFlows uploads** processed file back to storage
8. **FileFlows webhooks** n8n with completion status
9. **n8n updates** Directus with processed file URL

**Processing Capabilities:**
[Source: architecture/core-workflows.md#workflow-4-media-file-processing-pipeline]
- **Video transcoding:** H.264/H.265 encoding, resolution scaling, bitrate optimization
- **Audio processing:** Normalization (loudnorm), format conversion, silence removal
- **Image optimization:** WebP conversion, resizing, compression
- **Batch processing:** Queue multiple files with priority scheduling

### Environment Variables Required

**Core Configuration:**
[Source: architecture/development-workflow.md#required-environment-variables]

- `FILEFLOWS_HOST=fileflows.${BORGSTACK_DOMAIN}` - Service hostname for Caddy routing
- `TZ=America/Sao_Paulo` - Timezone for log timestamps
- `PUID=1000` - User ID for file permissions
- `PGID=1000` - Group ID for file permissions

**Processing Configuration:**
- `FILEFLOWS_LOG_LEVEL=Information` - Logging verbosity (Trace, Debug, Information, Warning, Error, Critical)
- `FILEFLOWS_LICENSE_KEY` - Optional commercial license key for advanced features

**Storage Configuration (Story 4.2 - Local Volumes):**
- Input directory: `/input` (borgstack_fileflows_input volume)
- Output directory: `/output` (borgstack_fileflows_output volume)
- Temp directory: `/temp` (borgstack_fileflows_temp volume)

### Coding Standards

**Docker Compose Configuration:**
[Source: architecture/coding-standards.md#critical-infrastructure-rules]

1. **Version Pinning:** Use exact version `revenz/fileflows:25.09` (NOT `latest`)
2. **Volume Naming:** `borgstack_fileflows_*` follows naming convention
3. **Network Isolation:** Connect to `borgstack_internal` (for future SeaweedFS access) and `borgstack_external` (Caddy)
4. **Health Checks:** Required - use HTTP check on port 5000 `/`
5. **Dependency Management:** Use `depends_on` with `service_started` for Caddy
6. **Configuration as Code:** Store FileFlows config in environment variables and mounted config files

**Security Requirements:**
[Source: architecture/coding-standards.md#critical-infrastructure-rules]

- Never expose port 5000 to host in production (proxy via Caddy only)
- Set appropriate file permissions with PUID/PGID
- Restrict access to FileFlows UI via Caddy authentication if needed

### Caddy Reverse Proxy Configuration

**Expected Caddyfile Entry:**
[Source: architecture/components.md#caddy]

```
{$FILEFLOWS_HOST} {
    reverse_proxy fileflows:5000

    # Increase timeout for long-running media processing requests
    @long_running {
        path /api/flow/*
    }
    reverse_proxy @long_running fileflows:5000 {
        timeout 3600s
    }

    # WebSocket support for real-time processing status
    @websockets {
        header Connection *Upgrade*
        header Upgrade websocket
    }
    reverse_proxy @websockets fileflows:5000
}
```

**Security Headers:** Automatic via Caddy defaults
**SSL/TLS:** Automatic via Let's Encrypt

### Project Structure

**Configuration Files:**
[Source: architecture/unified-project-structure.md]

```
borgstack/
├── config/fileflows/
│   ├── README.md                    # FileFlows setup documentation
│   ├── s3-storage.env.example       # S3 storage migration template for Story 5.1
│   └── example-flows.json           # Pre-configured processing flow templates
├── docs/03-services/
│   └── fileflows.md                 # Portuguese user guide
└── tests/deployment/
    └── verify-fileflows.sh          # Deployment validation script
```

### Testing

**Testing Philosophy:**
[Source: architecture/testing-strategy.md#testing-pyramid]

- No unit tests (FileFlows is pre-built)
- Focus on deployment validation
- Configuration verification
- Processing workflow testing

**Validation Commands:**
```bash
# Validate docker-compose configuration
docker compose config | grep -A 30 "fileflows:"

# Test web UI endpoint
curl -f https://${FILEFLOWS_HOST}/

# Test file processing
docker compose cp test.mp4 fileflows:/input/
# Monitor processing in FileFlows UI

# Verify storage volumes
docker volume inspect borgstack_fileflows_input
docker volume inspect borgstack_fileflows_output

# Check processing logs
docker compose logs fileflows | grep -i "processing"
```

**Test Script Location:** `tests/deployment/verify-fileflows.sh`

### Integration Points

**FileFlows Integration with Other Services:**
[Source: architecture/components.md#component-diagrams]

1. **SeaweedFS:** Reads/writes media files via S3 API (Story 5.1) or local volumes (Story 4.2)
2. **Caddy:** Reverse proxy for HTTPS access to FileFlows web UI
3. **Directus:** Can trigger FileFlows via n8n webhook when media uploaded to CMS
4. **n8n:** Orchestrates media processing workflows (Directus → FileFlows → Directus update)
5. **Duplicati:** Backs up FileFlows configuration and processed media library

**n8n Integration Example:**
[Source: architecture/core-workflows.md#workflow-4-media-file-processing-pipeline]

```javascript
// n8n HTTP Request Node to trigger FileFlows processing
{
  "method": "POST",
  "url": "http://fileflows:5000/api/flow/trigger",
  "body": {
    "filename": "{{ $json.filename }}",
    "source_path": "/input/{{ $json.filename }}",
    "flow_id": "video-h264-transcode"
  }
}
```

### Troubleshooting Notes

**Common Issues:**

1. **Processing Failures:**
   - Check FFmpeg codec support: `docker compose exec fileflows ffmpeg -codecs`
   - Verify input file is not corrupted: `ffprobe <file>`
   - Check temp directory has sufficient disk space
   - Review FileFlows logs: `docker compose logs fileflows --tail=200`

2. **Storage Permission Issues:**
   - Verify PUID/PGID match host user: `id -u && id -g`
   - Check volume permissions: `docker compose exec fileflows ls -la /input /output /temp`
   - Ensure volumes are writable from container

3. **Performance Issues:**
   - Reduce concurrent processing limit if CPU saturated
   - Consider hardware transcoding (GPU passthrough) for better performance
   - Monitor server resources during processing: `docker stats fileflows`

4. **Web UI Not Accessible:**
   - Verify Caddy routing: `docker compose logs caddy | grep fileflows`
   - Check FileFlows container is healthy: `docker compose ps fileflows`
   - Test direct container access: `curl http://localhost:5000` (from host with port mapped)

### Migration to SeaweedFS (Story 5.1)

**Migration Steps (To be executed during Story 5.1):**

1. Stop FileFlows container: `docker compose stop fileflows`
2. Copy existing media from local volumes to SeaweedFS:
   ```bash
   docker compose exec seaweedfs s3cmd put --recursive \
     /mnt/borgstack_fileflows_input/ \
     s3://borgstack/fileflows/input/
   ```
3. Update docker-compose.yml: Remove local volume mounts, add S3 environment variables
4. Apply S3 configuration from `config/fileflows/s3-storage.env.example`
5. Restart FileFlows: `docker compose up -d fileflows`
6. Verify S3 connectivity and processing works
7. Archive local volumes after verification: `docker volume rm borgstack_fileflows_input borgstack_fileflows_output`

## Testing

**Test File Location:**
[Source: architecture/testing-strategy.md#testing-pyramid]
- Deployment tests: `tests/deployment/verify-fileflows.sh`
- Integration tests: Not required for Story 4.2 (pre-built image)

**Testing Requirements:**
[Source: architecture/testing-strategy.md#testing-pyramid]
1. **Configuration Validation:**
   - Verify docker-compose.yml syntax: `docker compose config`
   - Validate environment variables in .env.example
   - Check Caddyfile routing for FileFlows host

2. **Deployment Validation:**
   - Container starts successfully and reaches healthy state
   - All volumes mounted correctly
   - Networks configured properly (borgstack_internal + borgstack_external)
   - Caddy reverse proxy routes traffic correctly
   - HTTPS certificate issued by Let's Encrypt

3. **Functional Testing:**
   - Web UI accessible via https://fileflows.${DOMAIN}/
   - Processing node initializes correctly
   - Input/output directories writable
   - Basic video transcoding flow completes successfully
   - Processed files appear in output directory

4. **Error Handling:**
   - Graceful handling of corrupted media files
   - Proper error logging for unsupported formats
   - Processing queue management under load

**Test Frameworks:**
- Bash scripts for deployment validation
- Manual testing for processing workflows (documented in README.md)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-05 | 1.0 | Initial story creation for FileFlows Media Processing deployment with local storage (SeaweedFS migration planned for Story 5.1) | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes

### File List

## QA Results
