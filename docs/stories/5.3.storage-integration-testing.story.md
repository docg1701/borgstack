# Story 5.3: Storage Integration Testing

## Status
Approved

## Story
**As a** quality assurance engineer,
**I want** all storage systems working together seamlessly,
**so that** applications can reliably store and retrieve data across all components.

## Acceptance Criteria
1. n8n configured to use SeaweedFS for workflow attachments
2. Directus configured to use SeaweedFS for asset storage
3. Basic S3 compatibility tested with standard S3 client
4. FileFlows uses SeaweedFS for input/output
5. Storage performance benchmarks established
6. Storage capacity monitoring implemented

## Tasks / Subtasks

- [ ] **Task 0: Verify SeaweedFS Prerequisites and Review Integration Requirements** (AC: 1, 2, 3, 4)
  - [ ] Verify SeaweedFS container is running and healthy from Story 5.1: `docker compose ps seaweedfs | grep "healthy"`
  - [ ] Verify S3 API endpoint accessible on `borgstack_internal`: `curl -f http://seaweedfs:8333`
  - [ ] Review SeaweedFS configuration from Story 5.1:
    - Master port: 9333
    - Volume port: 8080
    - Filer port: 8888
    - S3 API port: 8333
  - [ ] Review services that need SeaweedFS integration:
    - n8n (workflow attachments)
    - Directus (asset storage)
    - FileFlows (media processing input/output)
  - [ ] **CHECKPOINT:** Confirm SeaweedFS is operational before proceeding

- [ ] **Task 1: Configure n8n to Use SeaweedFS for Workflow Attachments** (AC: 1)
  - [ ] Review n8n S3 configuration requirements from architecture
  - [ ] Add n8n environment variables to `.env.example`:
    - `N8N_AVAILABLE_BINARY_DATA_MODES=filesystem,s3` (enable S3 mode)
    - `N8N_DEFAULT_BINARY_DATA_MODE=s3` (default to S3 storage)
    - `N8N_EXTERNAL_STORAGE_S3_HOST=seaweedfs:8333` (S3 endpoint with port)
    - `N8N_EXTERNAL_STORAGE_S3_BUCKET_NAME=n8n-workflows`
    - `N8N_EXTERNAL_STORAGE_S3_BUCKET_REGION=us-east-1` (required by n8n, ignored by SeaweedFS)
    - `N8N_EXTERNAL_STORAGE_S3_ACCESS_KEY=${SEAWEEDFS_S3_ACCESS_KEY}`
    - `N8N_EXTERNAL_STORAGE_S3_ACCESS_SECRET=${SEAWEEDFS_S3_SECRET_KEY}`
  - [ ] Update `docker-compose.yml` n8n service with S3 environment variables
  - [ ] Ensure n8n depends on SeaweedFS: `depends_on: seaweedfs: condition: service_healthy`
  - [ ] Create S3 bucket for n8n in SeaweedFS (via AWS CLI: `aws s3 mb s3://n8n-workflows --endpoint-url=http://localhost:8333`)
  - [ ] Test n8n S3 integration: Upload workflow attachment and verify storage in SeaweedFS
  - [ ] Document n8n S3 configuration in `config/n8n/README.md`

- [ ] **Task 2: Configure Directus to Use SeaweedFS for Asset Storage** (AC: 2)
  - [ ] Review Directus S3 storage driver configuration requirements
  - [ ] Add Directus S3 environment variables to `.env.example`:
    - `STORAGE_LOCATIONS=s3`
    - `STORAGE_S3_DRIVER=s3`
    - `STORAGE_S3_KEY=${SEAWEEDFS_S3_ACCESS_KEY}`
    - `STORAGE_S3_SECRET=${SEAWEEDFS_S3_SECRET_KEY}`
    - `STORAGE_S3_BUCKET=directus-assets`
    - `STORAGE_S3_ENDPOINT=http://seaweedfs:8333`
    - `STORAGE_S3_REGION=us-east-1` (SeaweedFS uses default region)
  - [ ] Update `docker-compose.yml` directus service with S3 environment variables
  - [ ] Ensure Directus depends on SeaweedFS: `depends_on: seaweedfs: condition: service_healthy`
  - [ ] Create S3 bucket for Directus in SeaweedFS
  - [ ] Test Directus S3 integration: Upload asset via Directus UI and verify storage in SeaweedFS
  - [ ] Document Directus S3 configuration in `config/directus/README.md`

- [ ] **Task 3: Test Basic S3 Compatibility with Standard S3 Client** (AC: 3)
  - [ ] Install AWS CLI v2 for S3 compatibility testing:
    - `curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"`
    - `unzip awscliv2.zip && sudo ./aws/install`
    - Verify: `aws --version` (should show aws-cli/2.x)
  - [ ] Configure S3 client with SeaweedFS endpoint and credentials:
    - `aws configure set aws_access_key_id ${SEAWEEDFS_S3_ACCESS_KEY}`
    - `aws configure set aws_secret_access_key ${SEAWEEDFS_S3_SECRET_KEY}`
    - `aws configure set default.region us-east-1`
    - `aws configure set default.s3.signature_version s3v4`
  - [ ] Create test bucket: `aws s3 mb s3://test-bucket --endpoint-url=http://localhost:8333`
  - [ ] Upload test file: `aws s3 cp test-file.txt s3://test-bucket/`
  - [ ] List bucket contents: `aws s3 ls s3://test-bucket/ --endpoint-url=http://localhost:8333`
  - [ ] Download test file: `aws s3 cp s3://test-bucket/test-file.txt downloaded.txt`
  - [ ] Verify file integrity: `diff test-file.txt downloaded.txt`
  - [ ] Delete test file and bucket: `aws s3 rb s3://test-bucket --force`
  - [ ] Document S3 compatibility testing in `docs/04-integrations/seaweedfs-s3-testing.md`

- [ ] **Task 4: Configure FileFlows to Use SeaweedFS for Input/Output** (AC: 4)
  - [ ] Review FileFlows storage configuration from Story 4.2 (uses Docker volume mounts)
  - [ ] Review media processing workflow from architecture (Workflow 4)
  - [ ] Create S3 buckets in SeaweedFS for media storage:
    - `aws s3 mb s3://fileflows-input --endpoint-url=http://localhost:8333` (raw uploads)
    - `aws s3 mb s3://fileflows-output --endpoint-url=http://localhost:8333` (processed media)
  - [ ] Mount SeaweedFS Filer directories as volumes in FileFlows container:
    - Add to `docker-compose.yml`: `- seaweedfs-filer:/buckets:ro` (read-only access to all buckets via Filer)
  - [ ] Configure FileFlows libraries via UI to point to mounted Filer paths:
    - Input Library: `/buckets/fileflows-input/` (monitors this directory)
    - Output Library: `/buckets/fileflows-output/` (writes processed files here)
  - [ ] Ensure FileFlows depends on SeaweedFS: `depends_on: seaweedfs: condition: service_healthy`
  - [ ] Test FileFlows integration:
    - Upload test video to `fileflows-input` bucket via S3 API or Filer
    - Trigger FileFlows processing workflow (should detect file in mounted volume)
    - Verify processed output appears in `fileflows-output` bucket
  - [ ] Document FileFlows SeaweedFS configuration in `config/fileflows/README.md`

- [ ] **Task 5: Establish Storage Performance Benchmarks** (AC: 5)
  - [ ] Review performance testing strategy from architecture/testing-strategy.md
  - [ ] Create performance benchmark script: `tests/integration/test-storage-performance.sh`
  - [ ] Generate test files:
    - `dd if=/dev/urandom of=test-10mb.dat bs=1M count=10` (10MB test file)
    - `dd if=/dev/urandom of=test-1gb.dat bs=1M count=1024` (1GB test file)
  - [ ] Benchmark Test 1: Sequential write performance
    - Loop 100 uploads: `for i in {1..100}; do time aws s3 cp test-10mb.dat s3://benchmark/test-$i.dat --endpoint-url=http://localhost:8333; done`
    - Calculate average upload time: `awk '{sum+=$1; count++} END {print sum/count}'`
    - Target: < 2s per 10MB file
  - [ ] Benchmark Test 2: Sequential read performance
    - Loop 100 downloads: `for i in {1..100}; do time aws s3 cp s3://benchmark/test-$i.dat /tmp/download-$i.dat --endpoint-url=http://localhost:8333; done`
    - Calculate average download time
    - Target: < 1s per 10MB file
  - [ ] Benchmark Test 3: Concurrent operations (requires GNU parallel)
    - Install if needed: `sudo apt-get install parallel`
    - Concurrent uploads: `seq 1 10 | parallel -j10 'aws s3 cp test-10mb.dat s3://benchmark/concurrent-{}.dat --endpoint-url=http://localhost:8333'`
    - Measure total time and calculate throughput: `(10 * 10MB) / total_seconds = MB/s`
  - [ ] Benchmark Test 4: Large file handling
    - Upload 1GB: `time aws s3 cp test-1gb.dat s3://benchmark/large-file.dat --endpoint-url=http://localhost:8333`
    - Download 1GB: `time aws s3 cp s3://benchmark/large-file.dat /tmp/large-download.dat --endpoint-url=http://localhost:8333`
    - Verify integrity: `md5sum test-1gb.dat /tmp/large-download.dat`
  - [ ] Document baseline performance metrics in `docs/04-integrations/storage-performance-benchmarks.md`

- [ ] **Task 6: Implement Storage Capacity Monitoring** (AC: 6)
  - [ ] Review SeaweedFS capacity monitoring capabilities (verified via context7)
  - [ ] Create monitoring script: `scripts/check-storage-capacity.sh`
  - [ ] Query SeaweedFS master API for cluster status:
    - API endpoint: `curl http://localhost:9333/cluster/status` (master port, not S3 port)
    - Extract: `Free` (available bytes), `Max` (total capacity), volumes count
    - Calculate used: `used_bytes = Max - Free`
    - Calculate percentage: `used_percent = (used_bytes / Max) * 100`
  - [ ] Query SeaweedFS Filer API for per-bucket usage:
    - List buckets: `curl http://localhost:8888/buckets/` (Filer API)
    - Get bucket info: `curl http://localhost:8888/buckets/{bucket-name}/` for each:
      - n8n-workflows bucket size
      - directus-assets bucket size
      - fileflows-input bucket size
      - fileflows-output bucket size
  - [ ] Implement storage alerts in monitoring script:
    - Warning threshold: 70% capacity used → exit code 1, log warning
    - Critical threshold: 85% capacity used → exit code 2, send alert
    - Output format: JSON with `{total_bytes, used_bytes, free_bytes, used_percent, buckets: [{name, size_bytes}]}`
  - [ ] Add storage monitoring to healthcheck script: `scripts/healthcheck.sh`
  - [ ] Document storage monitoring procedures in `docs/06-maintenance.md`

- [ ] **Task 7: Create Integration Test Suite** (AC: 1, 2, 3, 4)
  - [ ] Create comprehensive integration test: `tests/integration/test-storage-integration.sh`
  - [ ] Test 1: Verify SeaweedFS is healthy and S3 API accessible
  - [ ] Test 2: Verify all required S3 buckets exist
  - [ ] Test 3: Test n8n S3 integration (upload/download workflow attachment)
  - [ ] Test 4: Test Directus S3 integration (upload/download asset)
  - [ ] Test 5: Test FileFlows S3 integration (process media file)
  - [ ] Test 6: Test basic S3 client operations (create/list/delete bucket)
  - [ ] Test 7: Verify storage capacity monitoring data availability
  - [ ] Test 8: Verify all services can write to SeaweedFS concurrently
  - [ ] Make script executable: `chmod +x tests/integration/test-storage-integration.sh`
  - [ ] Document test execution in README: `tests/integration/README.md`

- [ ] **Task 8: Update CI/CD Workflow for Storage Integration Validation**
  - [ ] Add `validate-storage-integration` job to `.github/workflows/ci.yml`
  - [ ] Validate n8n S3 configuration in docker-compose.yml
  - [ ] Validate Directus S3 configuration in docker-compose.yml
  - [ ] Validate FileFlows storage configuration
  - [ ] Verify SeaweedFS dependency declarations in all services
  - [ ] Run configuration syntax checks
  - [ ] Document CI validation steps

- [ ] **Task 9: Create Storage Integration Documentation** (AC: 1, 2, 3, 4, 5, 6)
  - [ ] Create comprehensive Portuguese guide: `docs/04-integrations/storage-integration.md`
  - [ ] Document SeaweedFS architecture and S3 API compatibility
  - [ ] Document n8n S3 configuration and usage
  - [ ] Document Directus S3 configuration and asset management
  - [ ] Document FileFlows media processing with SeaweedFS
  - [ ] Document S3 client usage examples
  - [ ] Document storage performance benchmarks and expectations
  - [ ] Document storage capacity monitoring and alerts
  - [ ] Include troubleshooting guide for common storage issues
  - [ ] Add storage integration to main architecture diagram

## Dev Notes

### Previous Story Insights

**From Story 5.1 (SeaweedFS Object Storage):**
[Source: docs/stories/5.1.seaweedfs-object-storage.story.md#dev-agent-record]

- **SeaweedFS Deployment Mode**: Unified server mode (master + volume + filer + S3 in single container) successfully deployed
- **S3 API Endpoint**: Port 8333 exposed on `borgstack_internal` network for service integration
- **Network Configuration**: SeaweedFS on `borgstack_internal` only (no external exposure), accessed via service DNS name `seaweedfs`
- **Volumes Created**: `borgstack_seaweedfs_master`, `borgstack_seaweedfs_volume`, `borgstack_seaweedfs_filer`
- **Health Check**: HTTP check on master port 9333 with 30s interval, 10s timeout, 3 retries
- **S3 Credentials**: Generated via `SEAWEEDFS_S3_ACCESS_KEY` and `SEAWEEDFS_S3_SECRET_KEY` in `.env`

**From Story 5.2 (Duplicati Backup System):**
[Source: docs/stories/5.2.duplicati-backup-system.story.md#dev-agent-record]

- **Testing Philosophy**: Focus on deployment validation and integration verification (not unit tests)
- **CI/CD Pattern**: Add validation jobs to `.github/workflows/ci.yml` for configuration quality checks
- **Documentation Standard**: Comprehensive Portuguese documentation required for all integrations
- **Performance Benchmarking**: Establish baseline metrics and document targets vs acceptable thresholds

### Component Specifications

**Services Requiring SeaweedFS Integration:**
[Source: architecture/components.md]

**n8n (Workflow Automation):**
- **Purpose**: Store workflow attachments in S3-compatible storage
- **Integration Point**: External storage configuration via environment variables
- **S3 Configuration Prefix**: `N8N_EXTERNAL_STORAGE_S3_*`
- **Bucket Name**: `n8n-workflows` (suggested)
- **Access Pattern**: Read/write workflow attachments, execution artifacts

**Directus (Headless CMS):**
- **Purpose**: Store CMS assets (images, videos, documents) in object storage
- **Integration Point**: Storage driver configuration via environment variables
- **S3 Configuration Prefix**: `STORAGE_S3_*`
- **Bucket Name**: `directus-assets` (suggested)
- **Access Pattern**: Upload assets via CMS UI, serve assets via API
- **Dependencies**: PostgreSQL (`directus_db`), Redis (caching), SeaweedFS (asset storage)

**FileFlows (Media Processing):**
[Source: fileflows.com/docs verified via WebSearch, architecture/core-workflows.md]
- **Purpose**: Read raw media files from input storage, write processed files to output storage
- **Integration Point**: Docker volume mounts + Library configuration (NOT native S3 support)
- **Storage Architecture**: FileFlows uses local filesystem/volumes, NOT S3 buckets directly
- **SeaweedFS Integration Method**: Mount SeaweedFS Filer volumes into FileFlows container
  - SeaweedFS Filer exposes buckets as directories at `/buckets/{bucket-name}/`
  - Mount Filer volume in FileFlows: `- seaweedfs-filer:/buckets:ro`
  - Configure FileFlows Libraries to monitor mounted paths: `/buckets/fileflows-input/`
- **Bucket Names**: `fileflows-input`, `fileflows-output` (S3 buckets, accessed via Filer mount)
- **Access Pattern**: FileFlows monitors mounted directory for new files, processes, writes to output directory
- **Processing Workflow**: [Source: architecture/core-workflows.md#workflow-4-media-file-processing-pipeline]
  1. User uploads video to Directus → stored in SeaweedFS S3 bucket `fileflows-input`
  2. Directus triggers n8n webhook → n8n triggers FileFlows processing
  3. FileFlows detects file in mounted volume `/buckets/fileflows-input/`, processes (transcode)
  4. FileFlows writes to `/buckets/fileflows-output/` (SeaweedFS bucket via Filer)
  5. n8n updates Directus asset record with processed file URL

**Chatwoot (Customer Service):**
- **Note**: Chatwoot attachment storage configuration optional for this story
- **Current**: Uses local volumes (`chatwoot_storage`, `chatwoot_public`)
- **Future Enhancement**: Can be migrated to SeaweedFS S3 storage (deferred to post-MVP)

### API Specifications

**SeaweedFS S3 API:**
[Source: architecture/external-apis.md, architecture/components.md#seaweedfs]

- **S3 API Endpoint**: `http://seaweedfs:8333` (internal network)
- **Filer API Endpoint**: `http://seaweedfs:8888` (internal network)
- **Master API Endpoint**: `http://seaweedfs:9333` (internal network)
- **Authentication**: AWS Signature V4 with access key and secret key
- **Supported S3 Operations**:
  - Bucket operations: CreateBucket, DeleteBucket, ListBuckets, HeadBucket
  - Object operations: PutObject, GetObject, DeleteObject, ListObjects, HeadObject
  - Multipart uploads: InitiateMultipartUpload, UploadPart, CompleteMultipartUpload
- **S3 Compatibility Level**: Standard S3 API (compatible with AWS SDK, boto3, aws-cli, s3cmd)
- **Region**: Uses default region `us-east-1` (SeaweedFS ignores region but requires it in client config)

**n8n S3 External Storage:**
[Source: n8n-io/n8n-docs verified via context7]
- **Documentation**: https://docs.n8n.io/hosting/configuration/environment-variables/external-storage/
- **Required Environment Variables** (CORRECTED):
  - `N8N_AVAILABLE_BINARY_DATA_MODES=filesystem,s3` - Enable S3 storage mode (CRITICAL: required)
  - `N8N_DEFAULT_BINARY_DATA_MODE=s3` - Set S3 as default storage (CRITICAL: required)
  - `N8N_EXTERNAL_STORAGE_S3_HOST=seaweedfs:8333` - S3 endpoint with port (format: host:port)
  - `N8N_EXTERNAL_STORAGE_S3_BUCKET_NAME=n8n-workflows` - Target bucket name
  - `N8N_EXTERNAL_STORAGE_S3_BUCKET_REGION=us-east-1` - AWS region (required by n8n, ignored by SeaweedFS)
  - `N8N_EXTERNAL_STORAGE_S3_ACCESS_KEY=${SEAWEEDFS_S3_ACCESS_KEY}` - S3 access key (NOT "_ID" suffix)
  - `N8N_EXTERNAL_STORAGE_S3_ACCESS_SECRET=${SEAWEEDFS_S3_SECRET_KEY}` - S3 secret (NOT "_ACCESS_KEY")
- **Use Case**: Store workflow execution data, binary files, attachments
- **Note**: Without BINARY_DATA_MODES configured, n8n will NOT use S3 even if credentials are set

**Directus S3 Storage Driver:**
- **Documentation**: https://docs.directus.io/configuration/config-options.html#file-storage
- **Required Environment Variables**:
  - `STORAGE_LOCATIONS=s3` - Enable S3 storage location
  - `STORAGE_S3_DRIVER=s3` - Use S3 driver
  - `STORAGE_S3_KEY` - S3 access key
  - `STORAGE_S3_SECRET` - S3 secret key
  - `STORAGE_S3_BUCKET` - Target bucket name
  - `STORAGE_S3_ENDPOINT` - S3 endpoint URL (http://seaweedfs:8333)
  - `STORAGE_S3_REGION` - AWS region (us-east-1)
- **Use Case**: Store all uploaded assets (images, videos, documents) via CMS

### File Locations

**Docker Compose Configuration:**
[Source: architecture/unified-project-structure.md]
- `docker-compose.yml` - Add S3 environment variables to n8n, directus, fileflows services

**Configuration Files:**
```
config/n8n/
└── README.md                   # n8n S3 configuration guide

config/directus/
└── README.md                   # Directus S3 storage configuration

config/fileflows/
└── README.md                   # FileFlows S3 integration guide
```

**Integration Tests:**
```
tests/integration/
├── README.md                              # Test suite documentation
├── test-storage-integration.sh            # Comprehensive storage integration tests (8 tests)
└── test-storage-performance.sh            # Performance benchmarking suite (4 benchmarks)
```

**Scripts:**
```
scripts/
└── check-storage-capacity.sh              # Storage monitoring script
```

**Documentation:**
```
docs/04-integrations/
├── storage-integration.md                 # Portuguese comprehensive guide
├── seaweedfs-s3-testing.md               # S3 compatibility testing procedures
└── storage-performance-benchmarks.md      # Performance baselines and targets
```

**CI/CD:**
```
.github/workflows/ci.yml                   # Add validate-storage-integration job
```

### Storage Integration Workflow

**Media Processing Pipeline:**
[Source: architecture/core-workflows.md#workflow-4-media-file-processing-pipeline]

**Sequence:**
1. **User uploads video via Directus CMS**
   - Directus receives file upload
   - Directus stores file in SeaweedFS bucket `directus-assets` via S3 API
   - Directus saves asset metadata in PostgreSQL `directus_db`

2. **Directus triggers n8n webhook**
   - Directus sends POST to `https://n8n.${DOMAIN}/webhook/directus-upload`
   - Payload: `{filename, file_url, asset_id, mimetype}`

3. **n8n triggers FileFlows processing**
   - n8n receives webhook
   - n8n sends POST to FileFlows API: `/api/flow/trigger`
   - Payload: `{filename, source_path}`

4. **FileFlows processes media file**
   - FileFlows downloads file from SeaweedFS `fileflows-input` bucket
   - FileFlows detects format (FFprobe)
   - FileFlows transcodes (FFmpeg): `-c:v libx264 -crf 23 -preset medium -c:a aac`
   - FileFlows uploads processed file to SeaweedFS `fileflows-output` bucket

5. **n8n updates Directus with processed file**
   - FileFlows sends completion webhook to n8n: `/webhook/fileflows-complete`
   - n8n receives processed file metadata
   - n8n sends PATCH to Directus: `/items/assets/{id}` with processed file URL
   - Directus updates asset record in PostgreSQL

### Testing Requirements

**Testing Philosophy:**
[Source: architecture/testing-strategy.md]

BorgStack focuses on **deployment validation and integration verification**, not unit tests (pre-built Docker images).

**Test Coverage Requirements:**

**Integration Tests (Task 7):**
1. ✅ SeaweedFS container running and healthy
2. ✅ S3 API endpoint accessible on port 8333
3. ✅ n8n S3 environment variables configured
4. ✅ Directus S3 environment variables configured
5. ✅ FileFlows storage configuration valid
6. ✅ All required S3 buckets exist and accessible
7. ✅ n8n can upload/download workflow attachments to/from SeaweedFS
8. ✅ Directus can upload/download assets to/from SeaweedFS
9. ✅ FileFlows can read/write media files to/from SeaweedFS
10. ✅ Standard S3 client (aws-cli) can perform basic operations
11. ✅ Concurrent operations from multiple services work correctly
12. ✅ Storage capacity monitoring returns valid data

**Performance Benchmarks (Task 5):**
[Source: architecture/testing-strategy.md#performance-testing]

**Target Performance (36GB RAM, 8 vCPU server):**

| Test | Metric | Target (p95) | Acceptable (p99) | Critical Threshold |
|------|--------|--------------|------------------|--------------------|
| **Sequential Write** | Upload time per 10MB file | < 2s | < 3s | > 5s |
| **Sequential Read** | Download time per 10MB file | < 1s | < 2s | > 4s |
| **Concurrent Operations** | Throughput with 10 concurrent uploads | > 50 MB/s | > 30 MB/s | < 10 MB/s |
| **Large File Upload** | 1GB file upload time | < 30s | < 60s | > 120s |
| **Large File Download** | 1GB file download time | < 20s | < 40s | > 90s |
| **S3 API Latency** | CreateBucket/ListBuckets response time | < 100ms | < 200ms | > 500ms |

**Performance Testing Tools:**
```bash
# Sequential write/read tests
for i in {1..100}; do
  time curl -s -F "file=@test-10mb.mp4" \
    --aws-sigv4 "aws:amz:us-east-1:s3" \
    http://localhost:8333/test-bucket/ > /dev/null
done

# S3 client compatibility
aws s3 mb s3://test-bucket --endpoint-url=http://localhost:8333
aws s3 cp large-file.zip s3://test-bucket/ --endpoint-url=http://localhost:8333
aws s3 ls s3://test-bucket/ --endpoint-url=http://localhost:8333

# Concurrent operations (using GNU parallel)
seq 1 10 | parallel -j10 'aws s3 cp test-{}.mp4 s3://test-bucket/ --endpoint-url=http://localhost:8333'
```

**CI/CD Integration (Task 8):**
- Add `validate-storage-integration` job to `.github/workflows/ci.yml`
- Execute configuration checks on every commit
- Run deployment verification tests (configuration-only, not full integration tests)
- Validate environment variable references
- Verify service dependency declarations

### Storage Capacity Monitoring

**Monitoring Requirements (AC: 6):**
[Source: seaweedfs/seaweedfs verified via context7, architecture/testing-strategy.md]

**SeaweedFS Cluster Status API:**
- **Endpoint**: `http://seaweedfs:9333/cluster/status` (Master API, NOT S3 port)
- **Alternative**: `http://seaweedfs:9333/dir/status` for detailed topology
- **Returns**: JSON with cluster topology, volumes, storage capacity
- **Key Metrics**:
  - `Topology.Free` - Available storage space (bytes)
  - `Topology.Max` - Total storage capacity (bytes)
  - `Topology.DataCenters[].Racks[].DataNodes[].Volumes[]` - Volume list with sizes
- **Example Response**:
```json
{
  "IsLeader": true,
  "Leader": "localhost:9333",
  "Topology": {
    "DataCenters": [...],
    "Free": 85899345920,
    "Max": 107374182400
  }
}
```

**Per-Bucket Storage Usage:**
- **Filer API**: `http://seaweedfs:8888/buckets/` (list all buckets)
- **Bucket Detail**: `http://seaweedfs:8888/buckets/{bucket-name}/` (specific bucket info)
- **Returns**: Bucket metadata including size, file count
- **Note**: Buckets are stored under `/buckets/` prefix in Filer

**Monitoring Script Requirements:**
```bash
# scripts/check-storage-capacity.sh
# - Query cluster status from master API
# - Calculate total, used, available storage
# - Query each bucket size from filer API
# - Compare against warning (70%) and critical (85%) thresholds
# - Output formatted report
# - Exit code 0 = OK, 1 = Warning, 2 = Critical
```

**Alert Thresholds:**
- **Warning**: 70% storage capacity used → log warning
- **Critical**: 85% storage capacity used → alert admin (email/webhook)
- **Emergency**: 95% storage capacity used → prevent new uploads

### Security Considerations

**S3 Access Credentials:**
[Source: architecture/coding-standards.md#critical-infrastructure-rules]

- **CRITICAL**: Never commit S3 credentials to git repository
- Generate strong credentials: `openssl rand -base64 32`
- Store in `.env` file with 600 permissions
- Add to `.env.example` with placeholder values
- Required variables:
  - `SEAWEEDFS_S3_ACCESS_KEY` - S3 access key ID (alphanumeric, 20+ chars)
  - `SEAWEEDFS_S3_SECRET_KEY` - S3 secret access key (base64, 40+ chars)

**Network Security:**
[Source: architecture/coding-standards.md#network-isolation]

- SeaweedFS S3 API accessible ONLY on `borgstack_internal` network
- No direct port exposure to host (no `ports:` mapping in docker-compose.yml)
- Services access via internal DNS name `seaweedfs:8333`
- External access only via Caddy reverse proxy if web UI required (not for S3 API)

**Bucket Access Control:**
- All buckets readable/writable by any service with credentials (shared access model for MVP)
- Per-bucket ACLs optional for future enhancement (deferred to post-MVP)

### Coding Standards Compliance

**Volume Naming:**
[Source: architecture/coding-standards.md#naming-conventions]
- ✅ All SeaweedFS volumes follow `borgstack_seaweedfs_*` pattern (verified in Story 5.1)

**Network Configuration:**
[Source: architecture/coding-standards.md#critical-infrastructure-rules]
- ✅ SeaweedFS on `borgstack_internal` network only (verified in Story 5.1)
- ✅ No port exposure to host for S3 API (security requirement)

**Version Pinning:**
[Source: architecture/coding-standards.md#critical-infrastructure-rules]
- ✅ SeaweedFS image version pinned: `chrislusf/seaweedfs:3.97` (verified in Story 5.1)

**Health Check Requirements:**
[Source: architecture/coding-standards.md#critical-infrastructure-rules]
- ✅ SeaweedFS health check configured (verified in Story 5.1)
- Ensure all dependent services use `depends_on: seaweedfs: condition: service_healthy`

**Dependency Management:**
[Source: architecture/coding-standards.md#critical-infrastructure-rules]
- ✅ n8n must depend on SeaweedFS: `depends_on: seaweedfs: condition: service_healthy`
- ✅ Directus must depend on SeaweedFS: `depends_on: seaweedfs: condition: service_healthy`
- ✅ FileFlows must depend on SeaweedFS: `depends_on: seaweedfs: condition: service_healthy`

**Configuration as Code:**
[Source: architecture/coding-standards.md#configuration-as-code]
- Store all S3 configuration in `.env.example` and `docker-compose.yml`
- Document bucket names and access patterns in service README files
- Track all changes in git version control

### Project Structure Alignment

**No Conflicts Detected:**

All file paths and directory structures align with defined project structure:
- Integration tests in `tests/integration/`
- Configuration documentation in `config/{service}/README.md`
- User documentation in `docs/04-integrations/`
- Scripts in `scripts/`
- CI/CD in `.github/workflows/ci.yml`

## Testing

### Test File Locations
[Source: architecture/unified-project-structure.md]

```
tests/integration/
├── README.md                              # Test suite documentation
├── test-storage-integration.sh            # Main integration test suite (12 tests)
└── test-storage-performance.sh            # Performance benchmarking (4 benchmarks)
```

### Test Standards
[Source: architecture/testing-strategy.md]

**Testing Philosophy:**
- Focus on deployment validation and integration verification
- No unit tests (pre-built Docker images from upstream)
- Verify all acceptance criteria through automated tests
- Establish performance baselines for capacity planning

**Test Execution:**
```bash
# Integration tests
chmod +x tests/integration/test-storage-integration.sh
./tests/integration/test-storage-integration.sh

# Performance benchmarks
chmod +x tests/integration/test-storage-performance.sh
./tests/integration/test-storage-performance.sh
```

**Expected Output:**
```
========================================
Storage Integration Tests
========================================
Test 1/12: SeaweedFS healthy and accessible... PASS
Test 2/12: Required S3 buckets exist... PASS
Test 3/12: n8n S3 integration... PASS
Test 4/12: Directus S3 integration... PASS
Test 5/12: FileFlows S3 integration... PASS
Test 6/12: S3 client compatibility... PASS
Test 7/12: Storage capacity monitoring... PASS
Test 8/12: Concurrent write operations... PASS
Test 9/12: Sequential read performance... PASS
Test 10/12: Large file upload... PASS
Test 11/12: Storage quota enforcement... PASS
Test 12/12: Cross-service data access... PASS
========================================
All tests passed! Storage integration ready.
========================================
```

### Testing Frameworks and Patterns
[Source: architecture/testing-strategy.md]

**Bash Testing Framework:**
- Use standard bash test patterns
- Each test outputs PASS/FAIL with descriptive messages
- Exit code 0 = all tests pass, Exit code 1 = at least one test failed
- Test independence (each test can run standalone)

**Integration Test Pattern:**
```bash
#!/bin/bash
# tests/integration/test-storage-integration.sh

set -e

echo "========================================"
echo "Storage Integration Tests"
echo "========================================"

# Test 1: SeaweedFS health
if docker compose ps seaweedfs | grep -q "healthy"; then
  echo "✅ Test 1/12: SeaweedFS healthy... PASS"
else
  echo "❌ Test 1/12: SeaweedFS not healthy... FAIL"
  exit 1
fi

# Test 2: S3 API endpoint accessible
if curl -f http://localhost:8333 > /dev/null 2>&1; then
  echo "✅ Test 2/12: S3 API accessible... PASS"
else
  echo "❌ Test 2/12: S3 API not accessible... FAIL"
  exit 1
fi

# Test 3: n8n S3 configuration
N8N_S3_HOST=$(docker compose exec -T n8n env | grep N8N_EXTERNAL_STORAGE_S3_HOST)
if [ -n "$N8N_S3_HOST" ]; then
  echo "✅ Test 3/12: n8n S3 configured... PASS"
else
  echo "❌ Test 3/12: n8n S3 not configured... FAIL"
  exit 1
fi

# ... additional tests
```

**Performance Benchmark Pattern:**
```bash
#!/bin/bash
# tests/integration/test-storage-performance.sh

echo "========================================"
echo "Storage Performance Benchmarks"
echo "========================================"

# Benchmark 1: Sequential write
TOTAL_TIME=0
for i in {1..100}; do
  START=$(date +%s.%N)
  aws s3 cp test-10mb.mp4 s3://benchmark-bucket/test-$i.mp4 \
    --endpoint-url=http://localhost:8333 > /dev/null 2>&1
  END=$(date +%s.%N)
  ELAPSED=$(echo "$END - $START" | bc)
  TOTAL_TIME=$(echo "$TOTAL_TIME + $ELAPSED" | bc)
done
AVG_TIME=$(echo "scale=2; $TOTAL_TIME / 100" | bc)

if (( $(echo "$AVG_TIME < 2.0" | bc -l) )); then
  echo "✅ Benchmark 1: Sequential write: ${AVG_TIME}s (target < 2s)... PASS"
else
  echo "⚠️  Benchmark 1: Sequential write: ${AVG_TIME}s (target < 2s)... WARNING"
fi

# ... additional benchmarks
```

### Specific Testing Requirements for This Story

**AC: 1 - n8n SeaweedFS Integration:**
- Verify n8n environment variables configured in docker-compose.yml
- Create test workflow with file attachment
- Upload attachment and verify storage in SeaweedFS `n8n-workflows` bucket
- Download attachment and verify file integrity

**AC: 2 - Directus SeaweedFS Integration:**
- Verify Directus S3 storage driver configured
- Upload test asset via Directus UI (image, video, document)
- Verify asset stored in SeaweedFS `directus-assets` bucket
- Retrieve asset via Directus API and verify file integrity

**AC: 3 - S3 Client Compatibility:**
- Install AWS CLI v2 or s3cmd
- Configure client with SeaweedFS endpoint and credentials
- Test CreateBucket, PutObject, GetObject, ListObjects, DeleteObject operations
- Verify multipart upload support (upload file > 5GB in parts)

**AC: 4 - FileFlows SeaweedFS Integration:**
- Upload test video to `fileflows-input` bucket
- Trigger FileFlows processing workflow
- Verify processed file appears in `fileflows-output` bucket
- Verify original file can be deleted after processing

**AC: 5 - Performance Benchmarks:**
- Run performance test suite (`test-storage-performance.sh`)
- Measure and document actual performance vs targets
- Identify bottlenecks if performance below acceptable thresholds
- Document findings in `storage-performance-benchmarks.md`

**AC: 6 - Capacity Monitoring:**
- Run storage capacity check script
- Verify cluster status API returns valid data
- Verify per-bucket usage data available
- Test warning/critical threshold alerts

**CI/CD Validation:**
- Add `validate-storage-integration` job to `.github/workflows/ci.yml`
- Run configuration checks on every commit
- Execute static validation tests (configuration-only, not full integration)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-07 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-10-07 | 1.1 | CRITICAL corrections: Fixed n8n environment variables (verified via context7), corrected FileFlows integration method (volume mounts, not S3), added SeaweedFS monitoring API endpoints, added AWS CLI installation steps, added performance benchmark script examples | Sarah (Product Owner) |
| 2025-10-07 | 1.2 | Story approved - All critical issues resolved and verified against official documentation | Sarah (Product Owner) |

## Dev Agent Record

*This section will be populated by the development agent during implementation.*

### Agent Model Used

*To be filled during implementation*

### Debug Log References

*To be filled during implementation*

### Completion Notes List

*To be filled during implementation*

### File List

*To be filled during implementation*

## QA Results

*This section will be populated by the QA agent during review.*
